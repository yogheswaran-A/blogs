
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>SDE, Weiner Process, ITO’s Lemma and Reverse Time Equation &#8212; Blogs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '01_weiner_process';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Viewing Diffusion, Score, Rectified flow, Heirrachical VAEs From The Same Lens" href="02_Diffusion_models_Score_Models_ETC.html" />
    <link rel="prev" title="Blogs Landing Page" href="00_landing_page.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_landing_page.html">
  
  
  
  
  
  
    <p class="title logo__title">Blogs</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_landing_page.html">
                    Blogs Landing Page
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">SDE, Weiner Process, ITO’s Lemma and Reverse Time Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_Diffusion_models_Score_Models_ETC.html">Viewing Diffusion, Score, Rectified flow, Heirrachical VAEs From The Same Lens</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/yogheswaran-a/blogs" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/yogheswaran-a/blogs/issues/new?title=Issue%20on%20page%20%2F01_weiner_process.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/01_weiner_process.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>SDE, Weiner Process, ITO’s Lemma and Reverse Time Equation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-spaces-and-random-variables">Probability Spaces And Random Variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-process">Stochastic Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-time-weiner-process">Discrete Time Weiner Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walk">Random walk</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-symmetric-random-walk">Scaled Symmetric Random Walk</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-interesting-case-arises-as-n-to-infty"><strong>A Interesting Case Arises As <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weiner-process">Weiner process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#total-variation">Total Variation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-variation">Quadratic Variation.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ito-s-lemma">ITO’s Lemma</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ito-s-product-rule">ITO’s Product Rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ornstein-uhlenbeck-process">Ornstein-Uhlenbeck Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ornsteinuhlenbeck-ou-process-solution">Ornstein–Uhlenbeck (OU) Process Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-of-x-t">Mean of <span class="math notranslate nohighlight">\(X_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-of-x-t">Variance of <span class="math notranslate nohighlight">\(X_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code">Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph">Graph</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-process-with-affine-drift-co-efficients">Stochastic Process With Affine Drift Co-efficients.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reverse-time-equation">Reverse Time Equation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sde-weiner-process-ito-s-lemma-and-reverse-time-equation">
<h1>SDE, Weiner Process, ITO’s Lemma and Reverse Time Equation<a class="headerlink" href="#sde-weiner-process-ito-s-lemma-and-reverse-time-equation" title="Link to this heading">#</a></h1>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>I wrote this blog as I was trying to understand the math behind the diffusion model.Though one can understand the algorithm of differernt types of formulation under diffusion models such as herirachical VAE, Score based models, Rectiflow, flow models without knowing much about SDE, to understand in detail why they all fall under the same category I beleive a deeper understanding of weineer process, OU process is needed, especaily the stochastic calculas which is the key to ITO’s lemma, forward and backward equations.<br />
In this section a brief informal intro to  wiener process and why stocastic calculas is provided and in the later section a formal definition is tried to be given.</p>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Informally we can define stocashtic process to be a collection of random varibales <span class="math notranslate nohighlight">\(\{X_1 ,X_1,X_2,...\}\)</span> indexed by <span class="math notranslate nohighlight">\(t\)</span> (mostly time). At each time <span class="math notranslate nohighlight">\(t\)</span> the value is defined by the value <span class="math notranslate nohighlight">\(X_t\)</span>.  For example consider a ball moving in space at time <span class="math notranslate nohighlight">\(t\)</span>, if the position(<span class="math notranslate nohighlight">\(s_t\)</span>) of the ball after a time <span class="math notranslate nohighlight">\(dt\)</span> is influenced by a random variable <span class="math notranslate nohighlight">\(\epsilon_t\)</span>, the position at time <span class="math notranslate nohighlight">\(t + dt\)</span> would be random. The ball would be moving randomly in space, for each infinitesimally time <span class="math notranslate nohighlight">\(dt\)</span> the change in position would be described by <span class="math notranslate nohighlight">\(\epsilon_t\)</span>. That is:</p>
<div class="math notranslate nohighlight">
\[
ds_t = \epsilon_t * dt
\]</div>
<p>Notice that unlike normal ODE, it is hard to define the integration of such a process. At each step <span class="math notranslate nohighlight">\(dt\)</span> the change is random, so you cannot form a classical Riemann sum in the usual way, each increment <span class="math notranslate nohighlight">\(\epsilon_t\)</span> * <span class="math notranslate nohighlight">\(dt\)</span> is a random kick and the path <span class="math notranslate nohighlight">\(s_t\)</span>​ is almost surely nowhere differentiable. To make sense of:</p>
<div class="math notranslate nohighlight">
\[
s_t  =  s_0 + \int ϵ_t dt
\]</div>
<p>we need to <strong>stochastic calculas</strong>. To see the probability  distribution of positions at a given time step, where the ball might end up (final  distribution), how the trajectory evolves for all of this we need stochastic calculas.<br />
Morever, when we model a ball’s random motion, we’re really looking at an uncountable collection of random variables, one for each instant in a continuous time interval. In other words, each possible trajectory is a function   <span class="math notranslate nohighlight">\(s ⁣:[0,T]→R\)</span>
and there are uncountably many such functions. Moreover, each trajectory itself involves uncountably many infinitesimal time steps. Because of this double infinity (uncountably many paths, each with uncountably many increments), the naïve, undergraduate definition of probability assign a number to every subset of outcomes breaks down. If one tried to give every possible subset of <span class="math notranslate nohighlight">\(R\)</span> (or of the space of trajectories) a probability, one would immediately run into paradoxes like Banach–Tarski: for example, Banach–Tarski shows that you can partition a solid ball into finitely many non‐measurable pieces and reassemble them into two balls of the same volume as the original, so in this case assiging a probability of 1 to the initial solid ball won’t work. To avoid such contradictions, modern probability theory restricts attention to a carefully chosen sigma‐algebra of measurable sets (for instance, the Borel sigma‐algebra on <span class="math notranslate nohighlight">\(R\)</span>). In that way, every event we care about, the ball is in Region A at time <span class="math notranslate nohighlight">\(t\)</span>, its maximum displacement up to time <span class="math notranslate nohighlight">\(t\)</span> lies in some interval, etc., is guaranteed to be measurable, and every non‐measurable, paradoxical subset is simply excluded from the probability model. (We will start probability measure definition and other realted things to understad weiner process in the below sections).</p>
<p>Let’s get back to</p>
<div class="math notranslate nohighlight">
\[
ds_t = \epsilon_t * dt
\]</div>
<p>here if we chose <span class="math notranslate nohighlight">\(\epsilon\)</span> to be a gausian distribution with mean zero informally we will get a weiner process (<span class="math notranslate nohighlight">\(W_t\)</span>).</p>
<p>A Wiener process <span class="math notranslate nohighlight">\(W_t\)</span>​ is a continuous-time stochastic process with the following properties:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(W(0) = 0\)</span>. Weiner process starts with zero as the intial condition.</p></li>
<li><p>Time correlation is zero: The changes in the process over non-overlapping time intervals are independent.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ C[W_{t+\tau} - W_t, W_t] = 0 \quad \text{for } \tau &gt; 0 \]</div>
<div class="math notranslate nohighlight">
\[ C[W_{t+\tau} - W_t, W_t] = \lim_{T\to\infty} \frac{1}{T} \int_{0}^{T} W_t * (W_{t+\tau} - W_t)   dt \quad = 0  \quad \text{for } \tau &gt; 0\]</div>
<ol class="arabic simple" start="3">
<li><p>Gaussian increments: The change <span class="math notranslate nohighlight">\(W_{t+u} − W_t\)</span>​ is normally distributed.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ E[W_{t_{i+1}} - W_{t_i}] = 0 \quad \text{and}\]</div>
<div class="math notranslate nohighlight">
\[ \text{Var}[W_{t_{i+1}} - W_{t_i}] = t_{i+1} - t_i\]</div>
<ol class="arabic simple" start="4">
<li><p>Continuous paths: The process has continuous paths in time. There is no discontiuity in the paths.</p></li>
</ol>
<p>The stochastic equation one defined below is encountered more in computer science(rectified flow), physics and finance. The ultimate goal of this post is to introduce ITO’s lemma and use it to solve such process. At the end we solve a OU process.</p>
<div class="math notranslate nohighlight">
\[ dX(t) = \alpha(X,t)dt + \beta(X,t)dW_t \]</div>
<p>where, <span class="math notranslate nohighlight">\(\alpha(X,t)\)</span> represents the <strong>drift term</strong>, <span class="math notranslate nohighlight">\(\beta(X,t)\)</span> is the <strong>diffusion term</strong>, and <span class="math notranslate nohighlight">\(W_t\)</span> represents <strong>weiner process</strong>. This equation describes how a system’s state <span class="math notranslate nohighlight">\(X(t)\)</span> evolves under both deterministic forces and random fluctuations.</p>
</section>
<section id="probability-spaces-and-random-variables">
<h2>Probability Spaces And Random Variables<a class="headerlink" href="#probability-spaces-and-random-variables" title="Link to this heading">#</a></h2>
<p>As discussed above we need measure theoretic definition of probability to avoid getting into problems. Lets start with defining sigma algebra which is kinda similar to event space.</p>
<p><strong>Definition of a σ-algebra</strong></p>
<p>Let <span class="math notranslate nohighlight">\(\Omega\)</span> be a non-empty set, and let <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> be a collection of subsets of <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
<p>We say that <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is a <strong><span class="math notranslate nohighlight">\(\sigma\)</span>-algebra</strong> if it satisfies the following three properties:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>The empty set belongs to <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>:</strong>
<span class="math notranslate nohighlight">\(\emptyset \in \mathcal{F}\)</span></p></li>
<li><p><strong>Closed under complement:</strong>
Whenever a set <span class="math notranslate nohighlight">\(A\)</span> belongs to <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, its complement <span class="math notranslate nohighlight">\(A^c\)</span> also belongs to <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.
If <span class="math notranslate nohighlight">\(A \in \mathcal{F}\)</span>, then <span class="math notranslate nohighlight">\(A^c \in \mathcal{F}\)</span></p></li>
<li><p><strong>Closed under countable unions:</strong>
Whenever a sequence of sets <span class="math notranslate nohighlight">\(A_1, A_2, A_3, \ldots\)</span> belongs to <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, their union also belongs to <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.
If <span class="math notranslate nohighlight">\(A_1, A_2, A_3, \ldots \in \mathcal{F}\)</span>, then
<span class="math notranslate nohighlight">\(\bigcup_{n=1}^\infty A_n \in \mathcal{F}\)</span></p></li>
</ol>
</div></blockquote>
<p>The elements of a σ-algebra are called <strong>measurable sets</strong>, and the pair (<strong>Ω</strong>, <strong>𝓕</strong>) defines a <strong>measurable space</strong>. Here <strong>Ω</strong> is like sample space, <em>𝓕</em> is like the events.</p>
<p>Whenever we want to measure something( like probability) we want our event space <em>𝓕</em> which is the subset of <strong>Ω</strong> to be <strong>σ-algebra</strong> so that our measure won’t be ill-defined (we will see a example below). By defining our event space like this we are making sure that we add the individual parts of the event space we get a non-overlapping parts, like adding individual pieces of sphere to make up a unit volume.
For finite event spaces we normally use the power set <span class="math notranslate nohighlight">\(2^Ω\)</span> as the event space which has all the properties defined above.</p>
<p>Now lets define probability measure,</p>
<p><strong>Definition of a Probability Measure</strong></p>
<blockquote>
<div><p>A <strong>probability measure</strong> <strong>P</strong> on an event space <strong>𝓕</strong> is a function that satisfies the following properties:</p>
<ol class="arabic simple">
<li><p><strong>Maps events to the unit interval:</strong>
<span class="math notranslate nohighlight">\(P: \mathcal{F} \rightarrow [0, 1]\)</span></p></li>
<li><p><strong>Returns 0 for the empty set and 1 for the entire space:</strong>
<span class="math notranslate nohighlight">\(P(\emptyset) = 0\)</span> and <span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span></p></li>
<li><p><strong>Satisfies countable additivity:</strong><br />
For any countable collection of pairwise disjoint events <span class="math notranslate nohighlight">\(\{E_i\}_{i \in I}\)</span> (i.e., <span class="math notranslate nohighlight">\(E_i \cap E_j = \emptyset\)</span> for <span class="math notranslate nohighlight">\(i \neq j\)</span>), their probabilities add up:<br />
<span class="math notranslate nohighlight">\(P\left(\bigcup_{i \in I} E_i\right) = \sum_{i \in I} P(E_i) \)</span></p></li>
</ol>
</div></blockquote>
<p>How does this new definition of sample space and probability measure help us? Lets look at The Banach–Tarski paradox, it roughly states that that:</p>
<p><em>A solid sphere in 3D space can be decomposed into a finite number of disjoint subsets, which can be reassembled using only rotations and translations to form two identical spheres each the same size as the original.</em></p>
<p>This seems to contradict our intuitive understanding of volume and conservation. How can one break one ball into parts and rearrange them into two identical balls? The sets involved in the Banach–Tarski paradox are non-measurable, they cannot be assigned a meaningful volume using any countably additive measure like Lebesgue measure.</p>
<p>So how σ-algebras and probability measures help, Let’s look at the definitions.</p>
<ol class="arabic simple">
<li><p>The σ-algebra is closed under countable unions and complements.</p></li>
<li><p>This closure ensures internal consistency.</p></li>
<li><p>Only subsets of Ω that live in this structure can be measured.</p></li>
</ol>
<p>The Banach–Tarski sets cannot be constructed within a σ-algebra like the Borel σ-algebra.And moe so,</p>
<ol class="arabic simple" start="4">
<li><p>The probability measure <span class="math notranslate nohighlight">\(P:F→[0,1]P:F→[0,1]\)</span> is only defined on measurable sets (those in <span class="math notranslate nohighlight">\(𝓕\)</span>).</p></li>
<li><p>So, any event or set not in 𝓕 is simply not assigned a probability. It lies outside the scope of the theory.</p></li>
</ol>
<p>This avoids contradictions like “duplicating” probability mass, because one can only measure events where countable additivity holds.</p>
<p>Banach–Tarski fails here because it relies on unmeasurable subsets (not in 𝓕). There’s no way to define a probability measure on all subsets of ℝ³ that:</p>
<ol class="arabic simple">
<li><p>Is countably additive, and</p></li>
<li><p>Is rotation-invariant, and</p></li>
<li><p>Assigns the expected volume to regular sets,</p></li>
</ol>
<p>without contradicting Banach–Tarski. Hope the above explanation gave some intuition.  Lets define random variable.</p>
<hr class="docutils" />
<blockquote>
<div><p>A <strong>random variable</strong> <span class="math notranslate nohighlight">\(X\)</span> is a measurable function <span class="math notranslate nohighlight">\(X: \Omega \rightarrow E \subseteq \mathbb{R}\)</span> where:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> must be part of a measurable space, <span class="math notranslate nohighlight">\((E, \mathcal{S})\)</span> (recall: <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> defines a <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra on the set <span class="math notranslate nohighlight">\(E\)</span>). For finite or countably infinite values of <span class="math notranslate nohighlight">\(X\)</span>, we generally use the powerset of <span class="math notranslate nohighlight">\(E\)</span>. Otherwise, we will typically use the <strong>Borel set</strong> for uncountably infinite sets (e.g., the real numbers).</p></li>
<li><p>For all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>, the pre-image of <span class="math notranslate nohighlight">\(s\)</span> under <span class="math notranslate nohighlight">\(X\)</span> is in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>. More precisely:<br />
<span class="math notranslate nohighlight">\( \{X \in s\} := \{\omega \in \Omega | X(\omega) \in s\} \in \mathcal{F} \)</span></p></li>
</ol>
</div></blockquote>
<p>What this essitially does is that, it helps us to define probability measure for non-numericals (like proability of number of heads less than 3). It helps us map these kind of statement(from the sample space <span class="math notranslate nohighlight">\(\Omega\)</span>) to a real numbers(<span class="math notranslate nohighlight">\(E\)</span>). The second condition ensures that the numericals we are assiging to the events have well defined probability by making sure <span class="math notranslate nohighlight">\(s \in \mathcal{F}\)</span>. Also note that the <strong><span class="math notranslate nohighlight">\(\sigma (X) \in \mathcal{F} \)</span></strong>. What this means is that we can assign probability  to all the sets defined by the <strong><span class="math notranslate nohighlight">\(\sigma\)</span>-algebra</strong> of <span class="math notranslate nohighlight">\(X\)</span> since they are a subset of <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.</p>
</section>
<section id="stochastic-process">
<h2>Stochastic Process<a class="headerlink" href="#stochastic-process" title="Link to this heading">#</a></h2>
<p>Stochastic Process is defined as:</p>
<blockquote>
<div><p>Suppose that <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> is a probability space, and that <span class="math notranslate nohighlight">\(T \subset \mathbb{R}\)</span> is of infinite cardinality. Suppose further that for each <span class="math notranslate nohighlight">\(t \in T\)</span>, there is a random variable <span class="math notranslate nohighlight">\(X_t: \Omega \rightarrow \mathbb{R}\)</span> defined on <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span>. The function <span class="math notranslate nohighlight">\(X: T \times \Omega \rightarrow \mathbb{R}\)</span> defined by <span class="math notranslate nohighlight">\(X(t, \omega) = X_t(\omega)\)</span> is called a stochastic process with indexing set <span class="math notranslate nohighlight">\(T\)</span>, and is written as <span class="math notranslate nohighlight">\(X = \{X_t, t \in T\}\)</span>.</p>
</div></blockquote>
<p>Lets break it down. The elements of the sample space <span class="math notranslate nohighlight">\(\Omega\)</span> are infinite. For example, suppose our sample space is tossing of coins inifinite number of times, then <span class="math notranslate nohighlight">\(\omega\)</span> one of the elements can be an infinite sequence of only heads.<br />
The second thing is that previously we said stocashtic process to be a collection of random varibales <span class="math notranslate nohighlight">\(\{X_1 ,X_1,X_2,...\}\)</span> indexed by <span class="math notranslate nohighlight">\(t\)</span>, here <span class="math notranslate nohighlight">\(t\)</span> is continous and is inifinite and <span class="math notranslate nohighlight">\(X_t\)</span> takes both <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(\omega\)</span>. So, <span class="math notranslate nohighlight">\(X_t ( \omega)\)</span> takes a infinite element and maps it to real number[<span class="math notranslate nohighlight">\(\in \mathcal(F) \)</span>] so that we can measure the probability.</p>
<p>Note that here <span class="math notranslate nohighlight">\(X_t\)</span> is not restircted by time, meaning the probaility the random variable <span class="math notranslate nohighlight">\(X_t\)</span> assigns to <span class="math notranslate nohighlight">\(\omega\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> can depend on the future. For example let <span class="math notranslate nohighlight">\(\omega\)</span> be 4 heads and thereafter tails <span class="math notranslate nohighlight">\(\{H,H,H,H,T,...\}\)</span>, so at <span class="math notranslate nohighlight">\(t = 4\)</span>, the probability the random varibale <span class="math notranslate nohighlight">\(X_4\)</span> assings can depend of the future, meaning the occurences of tails. To restrict our time, to be causal we need something called adapted processes. Before definig it we need to see what filtration means.</p>
<p>If we have want to our random variable to not depend on the future, how could one define it? what restriction can be put in place?</p>
<ol class="arabic simple">
<li><p>Note that the events are defined by <span class="math notranslate nohighlight">\(\mathcal F\)</span>.</p></li>
<li><p>We need to make sure that the random variable <span class="math notranslate nohighlight">\(X_t\)</span> depend only on <span class="math notranslate nohighlight">\(\{\omega_1,\omega_2,\omega_3,\omega_4,...\omega_t\}\)</span></p></li>
<li><p>To make this happen we need to modify the event space such that at time <span class="math notranslate nohighlight">\(t\)</span> the event space has only events upto time <span class="math notranslate nohighlight">\(t\)</span> and not the future.</p></li>
<li><p>The events(<span class="math notranslate nohighlight">\(\mathcal F_t\)</span>) at time <span class="math notranslate nohighlight">\(t\)</span> must have events till <span class="math notranslate nohighlight">\(t\)</span>, at time <span class="math notranslate nohighlight">\(t+1\)</span> the events <span class="math notranslate nohighlight">\(\mathcal F_{t+1}\)</span> should have all the events till time t and events at time <span class="math notranslate nohighlight">\(t+1\)</span>.</p></li>
</ol>
<p>For the above we need a concept called filtration which is defined on our event space <span class="math notranslate nohighlight">\(\mathcal F\)</span> and our index <span class="math notranslate nohighlight">\(T\)</span>,</p>
<blockquote>
<div><p>A <strong>filtration</strong> <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is an ordered collection of sub-<span class="math notranslate nohighlight">\(\sigma\)</span>-algebras <span class="math notranslate nohighlight">\(\mathcal{F} := (\mathcal{F}_t)_{t \in T}\)</span> where <span class="math notranslate nohighlight">\(\mathcal{F}_t\)</span> is a sub-<span class="math notranslate nohighlight">\(\sigma\)</span>-algebra of <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{F}_{t_1} \subseteq \mathcal{F}_{t_2}\)</span> for all <span class="math notranslate nohighlight">\(t_1 \le t_2\)</span>.</p>
</div></blockquote>
<p>What this does are the above mentioned points, basically it breaks our <span class="math notranslate nohighlight">\(F\)</span> into partitions such that each partition <span class="math notranslate nohighlight">\(F_t\)</span> is a superset of <span class="math notranslate nohighlight">\(F_{t+1}\)</span>.</p>
<p>Now we can define <strong>Adapted process</strong>:</p>
<blockquote>
<div><p>A stochastic process <span class="math notranslate nohighlight">\(X_t: T \times \Omega\)</span> is <strong>adapted</strong> to the filtration <span class="math notranslate nohighlight">\((\mathcal{F}_t)_{t \in T}\)</span> if the random variable <span class="math notranslate nohighlight">\(X_t\)</span> is <span class="math notranslate nohighlight">\(\mathcal{F}_t\)</span> - measurable for all <span class="math notranslate nohighlight">\(t\)</span>.</p>
</div></blockquote>
<p>Thus <span class="math notranslate nohighlight">\(\sigma (X_t)\)</span> is only <span class="math notranslate nohighlight">\(F_t\)</span> measurable and <span class="math notranslate nohighlight">\(F_t\)</span> does not depend on the future.</p>
</section>
<section id="discrete-time-weiner-process">
<h2>Discrete Time Weiner Process<a class="headerlink" href="#discrete-time-weiner-process" title="Link to this heading">#</a></h2>
<section id="random-walk">
<h3>Random walk<a class="headerlink" href="#random-walk" title="Link to this heading">#</a></h3>
<p>A random walk is the random walk on the integer number line <span class="math notranslate nohighlight">\(\mathcal Z\)</span> which starts at 0, and at each step moves +1 or −1 with equal probability.</p>
<div class="math notranslate nohighlight">
\[X_{n+1}​=X_n​+ \epsilon_n \]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\epsilon = \begin{cases}
+1 &amp; p = 0.5 \\
-1 &amp; p = 0.5
\end{cases}
\end{split}\]</div>
<p>The symmetric random walk <span class="math notranslate nohighlight">\(S_n\)</span> is defined as:​</p>
<div class="math notranslate nohighlight">
\[S_n = \sum_{i=1}^n X_i\]</div>
</section>
<section id="scaled-symmetric-random-walk">
<h3>Scaled Symmetric Random Walk<a class="headerlink" href="#scaled-symmetric-random-walk" title="Link to this heading">#</a></h3>
<p>The scaled symetric random walk for an positive integer <span class="math notranslate nohighlight">\(n\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[ 
W_n(t) = \frac{1}{\sqrt{n}} S_{\lfloor nt \rfloor} = \frac{1}{\sqrt{n}} \sum_{i=1}^{\lfloor nt \rfloor} X_i, \quad t \in [0, T] \quad (1)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\{X_i\}\)</span> are i.i.d. random variables with</p>
<div class="math notranslate nohighlight">
\[
P(X_i = 1) = P(X_i = -1) = \frac{1}{2}, \quad \mathbb{E}[X_i] = 0, \quad \text{Var}(X_i) = 1.
\]</div>
<p>where <span class="math notranslate nohighlight">\(S_{nt}\)</span> is the simple random walk defined above and <span class="math notranslate nohighlight">\(t\)</span> is continous time. If <span class="math notranslate nohighlight">\(nt\)</span> is a integer then <span class="math notranslate nohighlight">\(W_n(t)\)</span> takes <span class="math notranslate nohighlight">\(S_{nt}\)</span>, else it will be an interpolation between the two consecutive <span class="math notranslate nohighlight">\(S_{nt}\)</span>. For example,lets look at <span class="math notranslate nohighlight">\(W_{10}(t)\)</span>. For <span class="math notranslate nohighlight">\(t = 0\)</span>, the <span class="math notranslate nohighlight">\(W_{10}(t)\)</span> will be zero, at <span class="math notranslate nohighlight">\(t = 10\)</span> it will be <span class="math notranslate nohighlight">\(\pm 1\)</span>, for <span class="math notranslate nohighlight">\(t &lt; 10\)</span> it will be a iterpolation between 0 and <span class="math notranslate nohighlight">\(\pm 1\)</span>.</p>
<p>Some properties of the Scaled Symmetric Random Walk:</p>
<p><strong>1. Independence of Increments</strong><br />
Let <span class="math notranslate nohighlight">\(0 = t_0 &lt; t_1 &lt; t_2 &lt; \dots &lt; t_m\)</span> be points in time, and define the increments of the scaled walk:</p>
<div class="math notranslate nohighlight">
\[
W_n(t_1) - W_n(t_0),\quad W_n(t_2) - W_n(t_1),\quad \dots,\quad W_n(t_m) - W_n(t_{m-1}).
\]</div>
<p>These increments are independent.</p>
<p><strong>Why?</strong></p>
<p>Each increment involves a sum over disjoint sets of the <span class="math notranslate nohighlight">\(X_i\)</span>’s:</p>
<div class="math notranslate nohighlight">
\[
W_n(t_{i+1}) - W_n(t_i) = \frac{1}{\sqrt{n}} \sum_{j = \lfloor nt_i \rfloor + 1}^{\lfloor nt_{i+1} \rfloor} X_j.
\]</div>
<p>Since the <span class="math notranslate nohighlight">\(X_j\)</span>’s are independent and the intervals do not overlap, the increments of the scaled walk are also independent.</p>
<p><strong>2. Expectation and Variance of Increments</strong><br />
Let us now compute the expectation and variance of the increment <span class="math notranslate nohighlight">\(W_n(t_{i+1}) - W_n(t_i)\)</span>.</p>
<p><strong>2.1 Expectation</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[W_n(t_{i+1}) - W_n(t_i)] = \frac{1}{\sqrt{n}} \sum_{j = \lfloor nt_i \rfloor + 1}^{\lfloor nt_{i+1} \rfloor} \mathbb{E}[X_j] = 0.
\]</div>
<p><strong>2.2 Variance</strong></p>
<div class="math notranslate nohighlight">
\[
\text{Var}(W_n(t_{i+1}) - W_n(t_i)) = \frac{1}{n} \sum_{j = \lfloor nt_i \rfloor + 1}^{\lfloor nt_{i+1} \rfloor} \text{Var}(X_j).
\]</div>
<p>Since <span class="math notranslate nohighlight">\(\text{Var}(X_j) = 1\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>, and the number of terms in the sum is approximately <span class="math notranslate nohighlight">\(n(t_{i+1} - t_i)\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(W_n(t_{i+1}) - W_n(t_i))  = \frac{1}{n} \sum_{j = \lfloor nt_i \rfloor + 1}^{\lfloor nt_{i+1} \rfloor} \text{Var}(X_j) = \frac{\lfloor nt_{i+1} \rfloor - \lfloor nt_i \rfloor}{n} \quad \approx \frac{1}{n} \cdot n(t_{i+1} - t_i) = t_{i+1} - t_i.
\]</div>
<section id="a-interesting-case-arises-as-n-to-infty">
<h4><strong>A Interesting Case Arises As <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</strong><a class="headerlink" href="#a-interesting-case-arises-as-n-to-infty" title="Link to this heading">#</a></h4>
<p>This is where the scaled symmetric random walk truly becomes fascinating, as it converges to a continuous-time stochastic process known as <strong>Brownian Motion</strong> (or the <strong>Wiener Process</strong>), lets see how and some of it’s properties.</p>
<p><strong>Continuity:</strong>
As <span class="math notranslate nohighlight">\(n \to \infty\)</span>, the number of steps <span class="math notranslate nohighlight">\(\lfloor nt \rfloor\)</span> for any given <span class="math notranslate nohighlight">\(t\)</span> becomes infinitely large. The discrete steps of the random walk become infinitesimally small in time and magnitude (due to the <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n}}\)</span> scaling). This effectively smooths out the random walk, and <span class="math notranslate nohighlight">\(W_n(t)\)</span> converges in distribution to a continuous path. We no longer need to interpolate between discrete steps, the process itself becomes continuous.</p>
<p><strong>Independence of Increments (in the limit):</strong>
This property is preserved in the limit. Brownian motion also has independent increments. This means that the future movements of a Brownian motion, given its current position, are independent of its past movements. This is a characteristic feature of many memoryless stochastic processes.</p>
<p><strong>Expectation in the Limit:</strong>
As derived above, <span class="math notranslate nohighlight">\(\mathbb{E}[W_n(t)] = \mathbb{E}[\frac{1}{\sqrt{n}}\sum_{j=1}^{\lfloor nt \rfloor} X_j] = 0\)</span>. This holds true as <span class="math notranslate nohighlight">\(n \to \infty\)</span>. So, for Brownian motion <span class="math notranslate nohighlight">\(B(t)\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[B(t)] = 0\)</span>. This reflects the no drift characteristic of standard Brownian motion, it’s equally likely to move up (+1) or down (-1) over any given time interval.</p>
<p><strong>Variance in the Limit:</strong>
As shown, <span class="math notranslate nohighlight">\(\text{Var}(W_n(t)) = \text{Var}(W_n(t) - W_n(0)) \approx t\)</span>. In the limit as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, this approximation becomes exact. Thus, for Brownian motion <span class="math notranslate nohighlight">\(B(t)\)</span>, <span class="math notranslate nohighlight">\(\text{Var}(B(t)) = t\)</span>. This is a defining characteristic, the variance of Brownian motion at time <span class="math notranslate nohighlight">\(t\)</span> is simply <span class="math notranslate nohighlight">\(t\)</span>. This means the spread of the process from its starting point grows linearly with time.</p>
<p><strong>Convergence to a Normal Distribution (Central Limit Theorem):</strong>
This is the most crucial point for understanding the link between the scaled random walk and Brownian motion. The <strong>Central Limit Theorem (CLT)</strong> states that the sum of a large number of independent and identically distributed (i.i.d.) random variables, when properly normalized, will be approximately normally distributed, regardless of the original distribution of the individual variables.</p>
<p>In our case, <span class="math notranslate nohighlight">\(W_n(t) = \frac{1}{\sqrt{n}} \sum_{j=1}^{nt} X_j\)</span> is a sum of <span class="math notranslate nohighlight">\(nt\)</span> i.i.d. random variables <span class="math notranslate nohighlight">\(X_j\)</span>, each with mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>. As <span class="math notranslate nohighlight">\(n \to \infty\)</span>, the number of terms <span class="math notranslate nohighlight">\(nt\)</span> also goes to infinity. Therefore, by the CLT, <span class="math notranslate nohighlight">\(W_n(t)\)</span> converges in distribution to a normal random variable.</p>
<p>Specifically, since <span class="math notranslate nohighlight">\(\mathbb{E}[W_n(t)] = 0\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(W_n(t)) \approx t\)</span>, as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, <span class="math notranslate nohighlight">\(W_n(t)\)</span> converges in distribution to a normal distribution with mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(t\)</span>, i.e <span class="math notranslate nohighlight">\(\mathcal{N}(0, t)\)</span>.</p>
</section>
</section>
</section>
<section id="weiner-process">
<h2>Weiner process<a class="headerlink" href="#weiner-process" title="Link to this heading">#</a></h2>
<p>A stochastic process <span class="math notranslate nohighlight">\(\{W(t), t \ge 0\}\)</span> is called a <strong>Wiener process</strong> (or <strong>standard Brownian motion</strong>) if it satisfies the following properties:</p>
<ol class="arabic simple">
<li><p><strong>Initial Condition:</strong> <span class="math notranslate nohighlight">\(W(0) = 0\)</span></p></li>
<li><p><strong>Independent Increments:</strong>
For all <span class="math notranslate nohighlight">\(0 \le t_0 &lt; t_1 &lt; \dots &lt; t_n\)</span>, the increments
<span class="math notranslate nohighlight">\(W(t_1) - W(t_0), W(t_2) - W(t_1), \ldots, W(t_n) - W(t_{n-1})\)</span>
are independent random variables.</p></li>
<li><p><strong>Stationary Increments:</strong>
For <span class="math notranslate nohighlight">\(s &lt; t\)</span>, the increment <span class="math notranslate nohighlight">\(W(t) - W(s) \sim \mathcal{N}(0, t - s)\)</span>, i.e., it is normally distributed with mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(t - s\)</span>.</p></li>
<li><p><strong>Continuity:</strong> The sample paths of <span class="math notranslate nohighlight">\(W(t)\)</span> are almost surely continuous, meaning:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ \mathbb{P} \left( W(t) \text{ is continuous in } t \right) = 1 \]</div>
<p>Additional Notes:</p>
<ul>
<li><p>It is a <strong>Gaussian process</strong>, meaning any finite collection of random variables <span class="math notranslate nohighlight">\((W(t_1), \dots, W(t_n))\)</span> has a multivariate normal distribution.</p></li>
<li><p>It has zero drift and unit volatility:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[W(t)] = 0, \quad \text{Var}(W(t)) = t\]</div>
</li>
</ul>
<p>One way to think of weiner process is to imagine watching the motion of a dust particle suspended in air under a microscope. The particle jiggles around in a random, constantly collides with air molecules. This chaotic, continuous, random motion is the physical inspiration for Brownian motion, and the Wiener process is the precise mathematical model of that behavior.</p>
<section id="total-variation">
<h3>Total Variation<a class="headerlink" href="#total-variation" title="Link to this heading">#</a></h3>
<p>Taking a look at total variation helps us  understand why weiner process is not differentiable.</p>
<p>For a function <span class="math notranslate nohighlight">\(f: [0, T] \to \mathbb{R}\)</span>, the <strong>total variation</strong> over <span class="math notranslate nohighlight">\([0, T]\)</span> is:</p>
<div class="math notranslate nohighlight">
\[ \text{TV}(f, [0,T]) = \sup_{\Pi} \sum_{i=0}^{n-1} |f(t_{i+1}) - f(t_i)| \]</div>
<p>where the supremum is taken over all partitions <span class="math notranslate nohighlight">\(\Pi = \{0 = t_0 &lt; t_1 &lt; \dots &lt; t_n = T\}\)</span> of the interval.</p>
<p>Total variation of a Wiener process (Brownian motion) over any interval <span class="math notranslate nohighlight">\([0,T]\)</span> is infinite almost surely.<br />
Proof:<br />
Let <span class="math notranslate nohighlight">\(\Delta t = t_{i+1} - t_i\)</span>, and remember that:
<span class="math notranslate nohighlight">\(W_{t_{i+1}} - W_{t_i} \sim \mathcal{N}(0, \Delta t)\)</span>.
So this increment is a normal random variable with, Mean: <span class="math notranslate nohighlight">\(0\)</span>, Variance: <span class="math notranslate nohighlight">\(\Delta t\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, \Delta t)\)</span>. We want:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[|X|]\]</div>
<p>Known Result: Expected Absolute Value of a Normal Variable
If <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, \sigma^2)\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}[|X|] = \sigma \sqrt{\dfrac{2}{\pi}} \]</div>
<p>In our case, <span class="math notranslate nohighlight">\(\sigma = \sqrt{\Delta t}\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[|W_{t_{i+1}} - W_{t_i}|] = \sqrt{\Delta t} \sqrt{\frac{2}{\pi}} = \sqrt{\frac{2 \Delta t}{\pi}} 
\]</div>
<p>Suppose we break <span class="math notranslate nohighlight">\([0, T]\)</span> into <span class="math notranslate nohighlight">\(n\)</span> equal intervals of length <span class="math notranslate nohighlight">\(\Delta t = T/n\)</span>.</p>
<p>For each tiny interval, the expected absolute change is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}\left[|W_{t_{i+1}} - W_{t_i}|\right] = \sqrt{\dfrac{2\Delta t}{\pi}} = \sqrt{\dfrac{2T}{n\pi}} 
\]</div>
<p>Summing this over all <span class="math notranslate nohighlight">\(n\)</span> intervals:</p>
<div class="math notranslate nohighlight">
\[
\text{Expected total variation} \approx n \cdot \sqrt{\dfrac{2T}{n\pi}} = \sqrt{\dfrac{n^2 \cdot 2T}{n\pi}} = \sqrt{\dfrac{2Tn}{\pi}} \to \infty \text{ as } n \to \infty
\]</div>
<p>As we use finer and finer partitions (smaller <span class="math notranslate nohighlight">\(\Delta t\)</span>), we are adding more and more small wobbles, and they add up to infinity.</p>
<p><strong>Comparison to Smooth Functions</strong>,
For a smooth function, like <span class="math notranslate nohighlight">\(f(t) = t\)</span> ( or any polynomial function):</p>
<ul class="simple">
<li><p>The absolute change in each small interval goes down linearly with <span class="math notranslate nohighlight">\(\Delta t\)</span>.</p></li>
<li><p>So the total sum over <span class="math notranslate nohighlight">\(n\)</span> intervals stays finite.</p></li>
</ul>
<p>Brownian motion is different:</p>
<ul class="simple">
<li><p>In each small interval, the change is random.</p></li>
<li><p>The size of the change shrinks like <span class="math notranslate nohighlight">\(\sqrt{\Delta t}\)</span>, which isn’t fast enough to make the total sum converge.</p></li>
</ul>
<p>Imagine zooming in on a Brownian path. Unlike a smooth curve:</p>
<ul class="simple">
<li><p>One doesn’t see a line, just more noise</p></li>
<li><p>The closer one zooms, the more erratic the behavior appears</p></li>
<li><p>So every level of detail contributes more total movement</p></li>
</ul>
<p>We are summing infinitely many tiny but not tiny enough movements, and they diverge Brownian motion has infinite total variation because it fluctuates so wildly and continuously, at every time scale, it adds up too much wiggle to be finite.
This is why classical calculus breaks down.</p>
</section>
<section id="quadratic-variation">
<h3>Quadratic Variation.<a class="headerlink" href="#quadratic-variation" title="Link to this heading">#</a></h3>
<p>The <strong>quadratic variation</strong> of <span class="math notranslate nohighlight">\(X\)</span> over <span class="math notranslate nohighlight">\([0,T]\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
[X]_T = \lim_{|\Pi| \to 0} \sum_{i=0}^{n-1} \left( X_{t_{i+1}} - X_{t_i} \right)^2
\]</div>
<p>Where  <span class="math notranslate nohighlight">\(\Pi = \{0 = t_0 &lt; t_1 &lt; \dots &lt; t_n = T\}\)</span> is a partition of <span class="math notranslate nohighlight">\([0,T]\)</span> and  <span class="math notranslate nohighlight">\(|\Pi| = \max_i (t_{i+1} - t_i)\)</span> is the mesh of the partition. The limit is taken in probability.</p>
<p><strong>For the smooth functions <span class="math notranslate nohighlight">\(f\)</span>, the <strong>quadratic variation</strong> is <span class="math notranslate nohighlight">\([f]_T = 0\)</span> for all <span class="math notranslate nohighlight">\(T \ge 0\)</span></strong></p>
<p>Proof:<br />
Taking any partition <span class="math notranslate nohighlight">\(\Pi = \{0 = t_0 &lt; t_1 &lt; \dots &lt; t_n = T\}\)</span>.
Since <span class="math notranslate nohighlight">\(f(t) = t\)</span>, the increments are:</p>
<div class="math notranslate nohighlight">
\[
f(t_{i+1}) - f(t_i) = t_{i+1} - t_i =: \Delta t_i
\]</div>
<p>Then the quadratic variation sum becomes:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=0}^{n-1} (f(t_{i+1}) - f(t_i))^2 = \sum_{i=0}^{n-1} (\Delta t_i)^2
\]</div>
<p>Now suppose the mesh of the partition is <span class="math notranslate nohighlight">\(|\Pi| = \max_i \Delta t_i\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=0}^{n-1} (\Delta t_i)^2 \le |\Pi| \sum_{i=0}^{n-1} \Delta t_i = |\Pi| \cdot T
\]</div>
<p>(because <span class="math notranslate nohighlight">\(\sum \Delta t_i = T\)</span>, the total length of the interval)</p>
<p>So:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=0}^{n-1} (\Delta t_i)^2 \le T \cdot |\Pi|
\]</div>
<p>Now take the limit as <span class="math notranslate nohighlight">\(|\Pi| \to 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
[f]_T = \lim_{|\Pi| \to 0} \sum_{i=0}^{n-1} (\Delta t_i)^2 \le \lim_{|\Pi| \to 0} T \cdot |\Pi| = 0
\]</div>
<p>So the quadratic variation of <span class="math notranslate nohighlight">\(f(t) = t\)</span> over any interval is zero.</p>
<p>Now lets compute it for weiner process, recall that weiner moition is higly irregular, the total variation is infinity, the paths are unpredictable. But if we compute the Quadratic variation of the brownian motion, we get something predictable and deterministic.</p>
<p><strong>For the Wiener process <span class="math notranslate nohighlight">\(W\)</span>, the <strong>quadratic variation</strong> is <span class="math notranslate nohighlight">\([W]_T = T\)</span> for all <span class="math notranslate nohighlight">\(T \ge 0\)</span> almost surely.</strong></p>
<p>Proof:
The quadratic variation for a partition is:</p>
<div class="math notranslate nohighlight">
\[
Q_\Pi = \sum_{j=0}^{n-1} (W(t_{j+1}) - W(t_j))^2 
\]</div>
<p>This quantity is a random variable since it depends on the particular outcome path of the Wiener process (recall quadratic variation is with respect to a particular realized path).</p>
<p>To prove the theorem, we need to show that the sampled quadratic variation converges to <span class="math notranslate nohighlight">\(T\)</span> as <span class="math notranslate nohighlight">\(|\Pi| \to 0\)</span>. This can be accomplished by showing <span class="math notranslate nohighlight">\(\mathbb{E}[Q_\Pi] = T\)</span> and <span class="math notranslate nohighlight">\(\text{Var}[Q_\Pi] \to 0\)</span>, which says that we will converge to <span class="math notranslate nohighlight">\(T\)</span> regardless of the path taken.</p>
<p>We know that each increment in the Wiener process is independent; thus their sums are the sums of the respective means and variances of each increment. So given that we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{E}[(W(t_{j+1}) - W(t_j))^2] = \mathbb{E}[W(t_{j+1}) - W(t_j)]^2 \quad - \quad 0\\
= \mathbb{E}[(W(t_{j+1}) - W(t_j))^2] - \mathbb{E}[W(t_{j+1}) - W(t_j)]^2 \quad \\
= \text{Var}[W(t_{j+1}) - W(t_j)] \\
= t_{j+1} - t_j
\end{split}\]</div>
<p>We can easily compute <span class="math notranslate nohighlight">\(\mathbb{E}[Q_\Pi]\)</span> as desired:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{E}[Q_\Pi] = \mathbb{E}\left[\sum_{j=0}^{n-1} (W(t_{j+1}) - W(t_j))^2\right] \\
= \sum_{j=0}^{n-1} \mathbb{E}[(W(t_{j+1}) - W(t_j))^2] \quad \\
= \sum_{j=0}^{n-1} (t_{j+1} - t_j) \\
= T \quad 
\end{split}\]</div>
<p>From here, we use the fact that the expected value of the fourth moment of a normal random variable with zero mean is three times its variance. Anticipating the quantity we’ll need to compute the variance, we have:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[(W(t_{j+1}) - W(t_j))^4] = 3\text{Var}[(W(t_{j+1}) - W(t_j))]^2 = 3(t_{j+1} - t_j)^2 \quad 
\]</div>
<p>Computing the variance of the quadratic variation for each increment:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{Var}[(W(t_{j+1}) - W(t_j))^2] \\
= \mathbb{E}[((W(t_{j+1}) - W(t_j))^2 - \mathbb{E}[(W(t_{j+1}) - W(t_j))^2])^2] \\
= \mathbb{E}[((W(t_{j+1}) - W(t_j))^2 - (t_{j+1} - t_j))^2] \\
= \mathbb{E}[W(t_{j+1}) - W(t_j))^4 - 2(t_{j+1} - t_j)\mathbb{E}[(W(t_{j+1}) - W(t_j))^2] + (t_{j+1} - t_j)^2] \\
= 3(t_{j+1} - t_j)^2 - 2(t_{j+1} - t_j)^2 + (t_{j+1} - t_j)^2 \\
= 2(t_{j+1} - t_j)^2
\end{split}\]</div>
<p>From here, we can finally compute the variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{Var}[Q_\Pi] = \sum_{j=0}^{n-1} \text{Var}[(W(t_{j+1}) - W(t_j))^2] \\
= \sum_{j=0}^{n-1} 2(t_{j+1} - t_j)^2 \\
\le \sum_{j=0}^{n-1} 2|\Pi|(t_{j+1} - t_j) \\
= 2|\Pi|T \quad
\end{split}\]</div>
<p>As <span class="math notranslate nohighlight">\(\lim_{|\Pi| \to 0} \text{Var}[Q_\Pi] = 0\)</span>, therefore we have shown that <span class="math notranslate nohighlight">\(\lim_{|\Pi| \to 0} Q_\Pi = T\)</span> as required.</p>
<p><strong>Quadratic variation</strong> of Brownian motion measures the accumulated squared fluctuations of the path.
Even though the path is rough and random, the squared fluctuations add up deterministically:</p>
<div class="math notranslate nohighlight">
\[
[W]_t = t
\]</div>
<p>Now if we step back and look at the big picture, this leads to one beautiful result. If we take almost any sample path of a Wiener process and keep dividing the time into smaller and smaller pieces, and then add up the square of the increments, the total will match the length of the time interval. Like, if you’re checking over 5 seconds, the sum will be 5. Very clean. It grows at 1 unit per unit of time.</p>
<p>Honestly, that’s kind of mind-blowing. Because the path itself is totally random, jumping randomly, but somehow this squared sum still behaves in a very regular and predictable way. And what’s even more surprising is that the quadratic variation is not zero, even though the path is continuous. Usually, we think if something is continuous and smooth, it shouldn’t change that much. But here, it’s continuous, just not differentiable anywhere. It looks smooth from far, but when we zoom in, it’s completely jagged.</p>
<p>Informally we will write the quadratic variation as:</p>
<div class="math notranslate nohighlight">
\[ \boxed{ dW(t)dW(t)=dt }\]</div>
<p>Please note that <span class="math notranslate nohighlight">\(dW\)</span> is normally distributed, so the above is formally valid only when we sum many of these small squared increments over time.</p>
<p><strong>Also we can write,</strong></p>
<div class="math notranslate nohighlight">
\[
\boxed{
dW(t)\,dt = 0
}
\]</div>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[[W(t),t]_T = 0 \Rightarrow dW(t)dt=0\]</div>
<p>Let <span class="math notranslate nohighlight">\(X(t)\)</span> and <span class="math notranslate nohighlight">\(Y(t)\)</span> be two processes. Their quadratic covariation over interval <span class="math notranslate nohighlight">\([0,T]\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
[X,Y]_T = \lim_{|\Pi| \to 0} \sum_{i=1}^{n} \left(X(t_i) - X(t_{i-1})\right) \left(Y(t_i) - Y(t_{i-1})\right)
\]</div>
<p>for a partition <span class="math notranslate nohighlight">\(\Pi = \{0 = t_0 &lt; t_1 &lt; \dots &lt; t_n = T\}\)</span></p>
<p>For</p>
<div class="math notranslate nohighlight">
\[ X(t)=W(t), Y(t)=t \]</div>
<div class="math notranslate nohighlight">
\[
[W, t]_T = \lim_{|\Pi| \to 0} \sum_{i=1}^{n} \left(W(t_i) - W(t_{i-1})\right) \left(t_i - t_{i-1}\right)
\]</div>
<p>Now notice:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(W(t_i) - W(t_{i-1}) \sim \mathcal{N}(0, t_i - t_{i-1})\)</span>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(t_i - t_{i-1}\)</span> is deterministic.</p></li>
</ul>
<p>So this is a sum of independent mean-zero random variables times deterministic numbers.
The expected value of each term is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}\left[(W(t_i) - W(t_{i-1}))(t_i - t_{i-1})\right] = \mathbb{E}[W(t_i) - W(t_{i-1})] \cdot (t_i - t_{i-1}) = 0
\]</div>
<p>And the variance of the entire sum becomes small as <span class="math notranslate nohighlight">\(|\Pi| \to 0\)</span>, because each increment is independent and scaled by vanishingly small <span class="math notranslate nohighlight">\((t_i - t_{i-1})\)</span>.</p>
<p>So:</p>
<div class="math notranslate nohighlight">
\[
[W, t]_T = 0 \quad
\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[
dX(t)\,dY(t) = d[X, Y](t)
\]</div>
<div class="math notranslate nohighlight">
\[
dW(t)\,dt = d[W(t), t] = 0
\]</div>
<p>because the process <span class="math notranslate nohighlight">\([W(t), t] \equiv 0\)</span>, i.e., it’s constant in time.</p>
<div class="math notranslate nohighlight">
\[
\boxed{
[W(t), t]_T = 0 \quad \Rightarrow \quad dW(t)\,dt = 0
}
\]</div>
<p>Also note that,</p>
<div class="math notranslate nohighlight">
\[
\boxed{
dt\,dt = 0
}
\]</div>
</section>
</section>
<section id="ito-s-lemma">
<h2>ITO’s Lemma<a class="headerlink" href="#ito-s-lemma" title="Link to this heading">#</a></h2>
<p>From this section onwarads, it’s going to be a little bit informal.</p>
<p>Most of the SDE’s we encounter(I have encountered) will be of the form:</p>
<div class="math notranslate nohighlight">
\[Y(t)=f(t,X(t))\]</div>
<p>Where,</p>
<div class="math notranslate nohighlight">
\[ dX(t) = \mu(t)dt + \sigma(t)dW_t \]</div>
<p><span class="math notranslate nohighlight">\(Y(t)\)</span> is a deterministic function of <span class="math notranslate nohighlight">\(X(t)\)</span> which is not deterministic. We would like to know how <span class="math notranslate nohighlight">\(Y(t)\)</span>  changes for very small time differentials <span class="math notranslate nohighlight">\(dt\)</span>. So we actually want to be able to define the following equation:</p>
<div class="math notranslate nohighlight">
\[dY_t=df(t,X_t)\]</div>
<p>ITO’s lemma provides us a solution to answer this, it states that:</p>
<div class="math notranslate nohighlight">
\[ 
\boxed{
 df(t,X(t)) = \left(\frac{\partial f}{\partial t} + \mu(t)\frac{\partial f}{\partial x} + \frac{\sigma^2(t)}{2} \frac{\partial^2 f}{\partial x^2}\right)dt + \frac{\partial f}{\partial x} \sigma(t)dW(t)
}
\]</div>
<p>given <span class="math notranslate nohighlight">\(f\)</span> is to be twice differentiable with respect to <span class="math notranslate nohighlight">\(X_t\)</span> and at least once differentiable with respect to <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Proof:</p>
<p>The first step is to define the Taylor expansion for <span class="math notranslate nohighlight">\(f(t,X_t)\)</span> in its general:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(t,X_t) \approx f(t_0,X_0) + \frac{\partial f(t_0,X_0)}{\partial t} \underbrace{(t-t_0)}_{\Delta t} + \frac{\partial f(t_0,X_0)}{\partial X_t} \underbrace{(X_t-X_0)}_{\Delta X_t} + \frac{1}{2} \frac{\partial^2 f(t_0,X_0)}{\partial X_t^2} \underbrace{(X_t-X_0)^2}_{\Delta X_t^2} \quad \\
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
= f(t_0,X_0) + \frac{\partial f(t_0,X_0)}{\partial t} \Delta t + \frac{\partial f(t_0,X_0)}{\partial X_t} \Delta X_t + \frac{1}{2} \frac{\partial^2 f(t_0,X_0)}{\partial X_t^2} \Delta X_t^2 
\]</div>
<p>Applying the limit: <span class="math notranslate nohighlight">\([t→t_0,X_t→X_0]\)</span></p>
<div class="math notranslate nohighlight">
\[
\lim_{t \to t_0, X_t \to X_0} f(t,X_t) - f(t_0,X_0) = \lim_{t \to t_0, X_t \to X_0} \frac{\partial f(t_0,X_0)}{\partial t} \Delta t + \frac{\partial f(t_0,X_0)}{\partial X_t} \Delta X_t + \frac{1}{2} \frac{\partial^2 f(t_0,X_0)}{\partial X_t^2} \Delta X_t^2
\]</div>
<div class="math notranslate nohighlight">
\[
df(t,X_t) = \frac{\partial f(t_0,X_0)}{\partial t} dt + \frac{\partial f(t_0,X_0)}{\partial X_t} dX_t + \frac{1}{2} \frac{\partial^2 f(t_0,X_0)}{\partial X_t^2} dX_t^2
\]</div>
<p>The next step is substituting <span class="math notranslate nohighlight">\( dX(t) = \mu(t)dt + \sigma(t)dW_t \)</span> into the equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
df(t,X_t) = \frac{\partial f(t_0,X_0)}{\partial t} dt + \frac{\partial f(t_0,X_0)}{\partial X_t} dX_t + \frac{1}{2} \frac{\partial^2 f(t_0,X_0)}{\partial X_t^2} dX_t^2 \\
= \frac{\partial f(t_0,X_0)}{\partial t} dt + \frac{\partial f(t_0,X_0)}{\partial X_t} (\mu(t)dt+\sigma(t)dW_t) + \frac{1}{2} \frac{\partial^2 f(t_0,X_0)}{\partial X_t^2} (\mu(t)dt+\sigma(t)dW_t)^2 \quad \\
= \frac{\partial f(t_0,X_0)}{\partial t} dt + \frac{\partial f(t_0,X_0)}{\partial X_t} (\mu(t)dt+\sigma(t)dW_t) \\
\quad + \frac{1}{2} \frac{\partial^2 f(t_0,X_0)}{\partial X_t^2} (\mu(t)^2 dt^2 + 2\mu(t)\sigma(t) dt dW_t + \sigma(t)^2 dW_t^2) \quad \\ 
= \frac{\partial f(t_0,X_0)}{\partial t} dt + \frac{\partial f(t_0,X_0)}{\partial X_t} (\mu(t)dt+\sigma(t)dW_t) \\
\quad + \frac{1}{2} \frac{\partial^2 f(t_0,X_0)}{\partial X_t^2} (\mu(t)^2 dt\,dt + 2\mu(t)\sigma(t) dt dW_t + \sigma(t)^2 dW_t\,dW_t) \quad \\
\end{split}\]</div>
<p>Recall from before that,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(dW(t)\,dt = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dt\,dt = 0 \quad \text{and,}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dW(t)\,dW(t) = 0\)</span></p></li>
</ul>
<p>Applying this we get,</p>
<div class="math notranslate nohighlight">
\[
= \underbrace{\left(\frac{\partial f(t_0, X_0)}{\partial t} + \frac{\partial f(t_0, X_0)}{\partial X_t} \mu(t) + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t^2} \sigma(t)^2 \right)}_{\text{deterministic}} dt + \underbrace{\frac{\partial f(t_0, X_0)}{\partial X_t} \sigma(t) dW_t}_{\text{stochastic}}
\]</div>
<div class="math notranslate nohighlight">
\[ 
\boxed{
df(t,X(t)) = \left(\frac{\partial f}{\partial t} + \mu(t)\frac{\partial f}{\partial x} + \frac{\sigma^2(t)}{2} \frac{\partial^2 f}{\partial x^2}\right)dt + \frac{\partial f}{\partial x} \sigma(t)dW(t)
}
\]</div>
<p>Now lets look into OU process and ITO’s lemma application.</p>
</section>
<section id="ito-s-product-rule">
<h2>ITO’s Product Rule<a class="headerlink" href="#ito-s-product-rule" title="Link to this heading">#</a></h2>
<p>If we have two stochastic process <span class="math notranslate nohighlight">\(U(t)\)</span> and <span class="math notranslate nohighlight">\(V(t)\)</span>, where</p>
<div class="math notranslate nohighlight">
\[ dU(t) = \alpha_u(U,t)dt + \beta_u(U,t)dW_t \]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[ dV(t) = \alpha_y(V,t)dt + \beta_v(V,t)dW_t \]</div>
<p>then,</p>
<div class="math notranslate nohighlight">
\[
\boxed{
d(U(t)V(t)) = U(t)\,dV(t) + V(t)\,dU(t) + dU(t)\,dV(t)
}
\]</div>
<p>Proof:
This is a direct application of ITO’s lemma,</p>
<p>We want to find <span class="math notranslate nohighlight">\(dY_t = d(U_t V_t)\)</span>. We can use Itô’s Lemma for a function <span class="math notranslate nohighlight">\(f(U,V)=UV\)</span>.</p>
<p>First, let’s find the partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial f}{\partial U} = V \\
\frac{\partial f}{\partial V} = U \\
\frac{\partial^2 f}{\partial U^2} = 0 \\
\frac{\partial^2 f}{\partial V^2} = 0 \\
\frac{\partial^2 f}{\partial U \partial V} = 1
\end{split}\]</div>
<p>Itô’s Lemma for a function of two Itô processes, <span class="math notranslate nohighlight">\(f(U_t,V_t)\)</span>, is given by:</p>
<div class="math notranslate nohighlight">
\[
df(U_t,V_t) = \frac{\partial f}{\partial U}dU_t + \frac{\partial f}{\partial V}dV_t + \frac{1}{2}\frac{\partial^2 f}{\partial U^2}(dU_t)^2 + \frac{1}{2}\frac{\partial^2 f}{\partial V^2}(dV_t)^2 + \frac{\partial^2 f}{\partial U \partial V}dU_t dV_t
\]</div>
<p>The higher order terms become zero as <span class="math notranslate nohighlight">\(dW^n\)</span> where <span class="math notranslate nohighlight">\(n &gt; 2\)</span> becomes zero, <span class="math notranslate nohighlight">\(dt^n\)</span> where n &gt;= 2 becomes zero, <span class="math notranslate nohighlight">\(dW^n * dt^k\)</span> for any n,k becomes zero. (See the quadratic variation section).</p>
<p>This simplies to,</p>
<div class="math notranslate nohighlight">
\[
\boxed{
d(U_t,V_t) = V_t dU_t + U_t dV_t + dU_t dV_t
}
\]</div>
<p>When <span class="math notranslate nohighlight">\(U_t\)</span> is a <em>determinitic funtion</em>, then the product  rule is ordinary chain rule,</p>
<div class="math notranslate nohighlight">
\[
\boxed{
d(U_t,V_t) = V_t dU_t + U_t dV_t
}
\]</div>
</section>
<section id="ornstein-uhlenbeck-process">
<h2>Ornstein-Uhlenbeck Process<a class="headerlink" href="#ornstein-uhlenbeck-process" title="Link to this heading">#</a></h2>
<p>The <strong>Ornstein–Uhlenbeck (OU) process</strong> is one of the most fundamental stochastic processes in continuous time. It models systems that evolve randomly, but with a natural tendency to drift back toward a long-term mean. Unlike standard Brownian motion, which wanders off indefinitely, the OU process is mean-reverting, it fluctuates around a central value, making it useful in modeling physical systems, finance, and in diffusion models.</p>
<p>Mathematically, the OU process is defined by the stochastic differential equation (SDE):</p>
<div class="math notranslate nohighlight">
\[
dX_t = \theta(\mu - X_t)\,dt + \sigma\,dW_t
\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> is the long-term mean toward which the process is pulled,</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta &gt; 0\)</span> is the rate of mean reversion,</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> controls the intensity of the randomness, and</p></li>
<li><p><span class="math notranslate nohighlight">\(W_t\)</span> is standard Brownian motion (Wiener process).</p></li>
</ul>
<p>The term <span class="math notranslate nohighlight">\(\theta(\mu - X_t) \, dt\)</span> causes the process to drift back toward <span class="math notranslate nohighlight">\(\mu\)</span> whenever it deviates, while <span class="math notranslate nohighlight">\(\sigma\,dW_t\)</span> injects randomness into the motion. This balance between deterministic pull and random noise gives the OU process its characteristic wiggly but stable behavior.</p>
<section id="ornsteinuhlenbeck-ou-process-solution">
<h3>Ornstein–Uhlenbeck (OU) Process Solution<a class="headerlink" href="#ornsteinuhlenbeck-ou-process-solution" title="Link to this heading">#</a></h3>
<p>Lets solve the Ornstein–Uhlenbeck (OU) process, defined by the stochastic differential equation (SDE):</p>
<div class="math notranslate nohighlight">
\[
dX_t = \theta(\mu - X_t)\,dt + \sigma\,dW_t
\]</div>
<p>This is a linear stochastic differential equation, and it matches the form:</p>
<div class="math notranslate nohighlight">
\[
dX_t = -aX_t\,dt + \sigma\,dW_t
\]</div>
<p>except with an inhomogeneous drift <span class="math notranslate nohighlight">\(\theta\mu\)</span>. So let’s proceed similarly.</p>
<div class="math notranslate nohighlight">
\[
dX_t = \theta\mu\,dt - \theta X_t\,dt + \sigma\,dW_t = -\theta X_t\,dt + \theta\mu\,dt + \sigma\,dW_t
\]</div>
<p>Let’s apply the integrating factor method with <span class="math notranslate nohighlight">\(e^{\theta t}\)</span>. Multiply both sides by <span class="math notranslate nohighlight">\(e^{\theta t}\)</span></p>
<p>Let <span class="math notranslate nohighlight">\(Y_t = e^{\theta t}X_t\)</span>. Then using Itô’s product rule:</p>
<div class="math notranslate nohighlight">
\[
dY_t = d(e^{\theta t} X_t) = e^{\theta t} dX_t + \theta e^{\theta t} X_t\,dt
\]</div>
<p>Now substitute for <span class="math notranslate nohighlight">\(dX_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
dY_t = e^{\theta t}(-\theta X_t\,dt + \theta\mu\,dt + \sigma\,dW_t) + \theta e^{\theta t} X_t\,dt
\]</div>
<p>Now cancel the <span class="math notranslate nohighlight">\(-\theta X_t\,dt\)</span> and <span class="math notranslate nohighlight">\(+\theta X_t\,dt\)</span> terms:</p>
<div class="math notranslate nohighlight">
\[
dY_t = \theta\mu e^{\theta t}\,dt + \sigma e^{\theta t}\,dW_t
\]</div>
<p>Integrate both sides</p>
<div class="math notranslate nohighlight">
\[
Y_t - Y_0 = \int_0^t \theta\mu e^{\theta s}\,ds + \int_0^t \sigma e^{\theta s}\,dW_s
\]</div>
<p>Since <span class="math notranslate nohighlight">\(Y_0 = X_0\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[
e^{\theta t} X_t = X_0 + \theta\mu \int_0^t e^{\theta s}\,ds + \sigma \int_0^t e^{\theta s}\,dW_s
\]</div>
<p>Solve for <span class="math notranslate nohighlight">\(X_t\)</span></p>
<p>We divide both sides by <span class="math notranslate nohighlight">\(e^{\theta t}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
X_t = e^{-\theta t} X_0 + \mu(1 - e^{-\theta t}) + \sigma e^{-\theta t} \int_0^t e^{\theta s}\,dW_s
\]</div>
<p>This is the classic Ornstein–Uhlenbeck process solution.</p>
<div class="math notranslate nohighlight">
\[
\boxed{
X_t = e^{-\theta t} X_0 + \mu(1 - e^{-\theta t}) + \sigma e^{-\theta t} \int_0^t e^{\theta s}\,dW_s
}
\]</div>
<p>Let’s see how the mean and variance of the Ornstein–Uhlenbeck process looks like.</p>
<div class="math notranslate nohighlight">
\[
X_t = e^{-\theta t} X_0 + \mu(1 - e^{-\theta t}) + \sigma e^{-\theta t} \int_0^t e^{\theta s}\,dW_s
\]</div>
<p>This expression consists of three terms:</p>
<ul class="simple">
<li><p><strong>Deterministic term</strong>: <span class="math notranslate nohighlight">\(e^{-\theta t} X_0\)</span></p></li>
<li><p><strong>Drift toward mean <span class="math notranslate nohighlight">\(\mu\)</span></strong>: <span class="math notranslate nohighlight">\(\mu(1 - e^{-\theta t})\)</span></p></li>
<li><p><strong>Stochastic integral</strong>: <span class="math notranslate nohighlight">\(\sigma e^{-\theta t} \int_0^t e^{\theta s}\,dW_s\)</span></p></li>
</ul>
</section>
<section id="mean-of-x-t">
<h3>Mean of <span class="math notranslate nohighlight">\(X_t\)</span><a class="headerlink" href="#mean-of-x-t" title="Link to this heading">#</a></h3>
<p>We take the expectation of <span class="math notranslate nohighlight">\(X_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X_t] = \mathbb{E}[e^{-\theta t} X_0] + \mu(1 - e^{-\theta t}) + \sigma e^{-\theta t} \mathbb{E}\left[\int_0^t e^{\theta s}\,dW_s\right]
\]</div>
<p>Assuming <span class="math notranslate nohighlight">\(X_0\)</span> is a constant or an initial random variable with a finite expectation, <span class="math notranslate nohighlight">\(\mathbb{E}[e^{-\theta t} X_0] = e^{-\theta t} \mathbb{E}[X_0]\)</span>.
The expectation of the stochastic integral <span class="math notranslate nohighlight">\(\int_0^t f(s)\,dW_s\)</span> is zero when <span class="math notranslate nohighlight">\(f(s)\)</span> is deterministic, <span class="math notranslate nohighlight">\(dW_t\)</span> is brownian and has zero mean. Thus, <span class="math notranslate nohighlight">\(\mathbb{E}\left[\int_0^t e^{\theta s}\,dW_s\right] = 0\)</span>.</p>
<p>So:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X_t] = e^{-\theta t} \mathbb{E}[X_0] + \mu(1 - e^{-\theta t})
\]</div>
<p><strong>Mean:</strong></p>
<div class="math notranslate nohighlight">
\[
\boxed{
\mathbb{E}[X_t] = \mathbb{E}[X_0]\, e^{-\theta t} + \mu(1 - e^{-\theta t})
}
\]</div>
</section>
<section id="variance-of-x-t">
<h3>Variance of <span class="math notranslate nohighlight">\(X_t\)</span><a class="headerlink" href="#variance-of-x-t" title="Link to this heading">#</a></h3>
<p>Now we compute the variance. Since the first two terms (<span class="math notranslate nohighlight">\(e^{-\theta t} X_0\)</span> and <span class="math notranslate nohighlight">\(\mu(1 - e^{-\theta t})\)</span>) are deterministic (or at least uncorrelated with the stochastic integral if <span class="math notranslate nohighlight">\(X_0\)</span> is independent), and the stochastic integral has zero mean, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{Var}(X_t) = \operatorname{Var}\left( \sigma e^{-\theta t} \int_0^t e^{\theta s}\,dW_s \right) \\
= \sigma^2 e^{-2\theta t} \operatorname{Var} \left( \int_0^t e^{\theta s}\,dW_s \right)
\end{split}\]</div>
<p>We can write,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Var} \left( \int_0^t e^{\theta s}\,dW_s \right) = \mathbb{E} \left[ \left( \int_0^t e^{\theta s}\,dW_s \right)^2 \right] = \int_0^t (e^{\theta s})^2 \, dWdW = \int_0^t (e^{\theta s})^2 \, ds = \int_0^t e^{2\theta s} \, ds \quad
\]</div>
<p>Recall that, <span class="math notranslate nohighlight">\(dW\,dW = dt = ds\)</span> which is applied in the above equation.</p>
<p>Now, evaluate the definite integral:</p>
<div class="math notranslate nohighlight">
\[
\int_0^t e^{2\theta s}\,ds = \left[ \frac{e^{2\theta s}}{2\theta} \right]_0^t = \frac{e^{2\theta t}}{2\theta} - \frac{e^{2\theta \cdot 0}}{2\theta} = \frac{e^{2\theta t} - 1}{2\theta}
\]</div>
<p>Substitute this back into the variance expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{Var}(X_t) = \sigma^2 e^{-2\theta t} \left( \frac{e^{2\theta t} - 1}{2\theta} \right) \\
= \frac{\sigma^2}{2\theta} e^{-2\theta t} (e^{2\theta t} - 1) \\
= \frac{\sigma^2}{2\theta} (e^{-2\theta t} e^{2\theta t} - e^{-2\theta t}) \\
= \frac{\sigma^2}{2\theta} (1 - e^{-2\theta t})
\end{split}\]</div>
<p><strong>Variance:</strong></p>
<div class="math notranslate nohighlight">
\[
\boxed{
\operatorname{Var}(X_t) = \frac{\sigma^2}{2\theta} (1 - e^{-2\theta t})
}
\]</div>
<p><strong>Mean:</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X_t] = \mathbb{E}[X_0]\, e^{-\theta t} + \mu(1 - e^{-\theta t})
\]</div>
<p><strong>Variance:</strong></p>
<div class="math notranslate nohighlight">
\[
\operatorname{Var}(X_t) = \frac{\sigma^2}{2\theta} (1 - e^{-2\theta t})
\]</div>
<p>As <span class="math notranslate nohighlight">\(t \to \infty\)</span>, we get:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[X_t] \to \mu\)</span> (since <span class="math notranslate nohighlight">\(e^{-\theta t} \to 0\)</span> as <span class="math notranslate nohighlight">\(\theta &gt; 0\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{Var}(X_t) \to \frac{\sigma^2}{2\theta}\)</span> (since <span class="math notranslate nohighlight">\(e^{-2\theta t} \to 0\)</span> as <span class="math notranslate nohighlight">\(\theta &gt; 0\)</span>)</p></li>
</ul>
<p>This shows that the OU process converges in distribution to a stationary Gaussian distribution:</p>
<div class="math notranslate nohighlight">
\[
X_t \overset{d}{\to} \mathcal{N}\left(\mu, \frac{\sigma^2}{2\theta}\right)
\]</div>
</section>
<section id="code">
<h3>Code<a class="headerlink" href="#code" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># simulating OU process</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Parameters for the OU process</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.7</span>      <span class="c1"># rate of mean reversion</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.0</span>         <span class="c1"># Long-term mean</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.3</span>      <span class="c1"># intensity of the randomness</span>
<span class="n">X0</span> <span class="o">=</span> <span class="mf">1.0</span>         <span class="c1"># Initial value</span>
<span class="n">T</span> <span class="o">=</span> <span class="mf">10.0</span>         <span class="c1"># Total time</span>
<span class="n">dt</span> <span class="o">=</span> <span class="mf">0.01</span>        <span class="c1"># Time step</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">T</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>  <span class="c1"># Number of time steps</span>

<span class="c1"># Pre-allocate array for efficiency</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X0</span>

<span class="c1"># Generate the OU process</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">dt</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">dW</span>

<span class="c1"># Plot the result</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Ornstein-Uhlenbeck Process Simulation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;X(t)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="graph">
<h3>Graph<a class="headerlink" href="#graph" title="Link to this heading">#</a></h3>
<p><img alt="img" src="_images/OU_simu.png" /></p>
</section>
</section>
<section id="stochastic-process-with-affine-drift-co-efficients">
<h2>Stochastic Process With Affine Drift Co-efficients.<a class="headerlink" href="#stochastic-process-with-affine-drift-co-efficients" title="Link to this heading">#</a></h2>
<p>In the last section we saw that OU process converges to a Normal distribution.</p>
<p>This might make one ask a question, at any time <span class="math notranslate nohighlight">\(t\)</span>, can we have a <span class="math notranslate nohighlight">\(X_t\)</span> follow a normal distribution <span class="math notranslate nohighlight">\(X_t \sim \mathcal{N}\)</span> or <span class="math notranslate nohighlight">\(X_t/X_0 \sim \mathcal{N}\)</span>, as you might have expected the answer is YES, if the drift co-efficient is affine, i,e For a SDE</p>
<div class="math notranslate nohighlight">
\[ dX(t) = f(X,t)dt + \beta(X,t)dW_t \]</div>
<div class="math notranslate nohighlight">
\[X_t/X_0 \sim \mathcal{N}\]</div>
<p>If</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(X,t)=a(t)X+b(t) \quad \text{(i.e, Affine Drift Co-efficients) and } \\
\beta(X,t) = \beta(t) \quad \text{(i.e, diffusion coefficient is independent of X)}.
\end{split}\]</div>
<p>Proof:</p>
<p>Given a linear SDE of the form:</p>
<div class="math notranslate nohighlight">
\[
dX(t) = a(t)X(t)\,dt + b(t)\,dt + \beta(t)\,dW_t
\]</div>
<p>This can be rewritten as:</p>
<div class="math notranslate nohighlight">
\[
dX(t) = \left[a(t)X(t) + b(t)\right]dt + \beta(t)\,dW_t
\]</div>
<p>This is a linear SDE (i.e., linear in <span class="math notranslate nohighlight">\(X\)</span>), which can be solved using the method of integrating factors.
Let us define an integrating factor:</p>
<div class="math notranslate nohighlight">
\[
\mu(t) = \exp\left( -\int_0^t a(s) ds \right)
\]</div>
<p>Multiplying both sides of the SDE by <span class="math notranslate nohighlight">\(\mu(t)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mu(t)\,dX(t) = \mu(t)\left[a(t)X(t) + b(t)\right]dt + \mu(t)\beta(t)\,dW_t
\]</div>
<p>Using Ito’s product rule (recall that since <span class="math notranslate nohighlight">\(\mu_t\)</span> is determisnistic it is just chain rule):</p>
<div class="math notranslate nohighlight">
\[
d[\mu(t)X(t)] = \mu(t)\,dX(t) + X(t)\,d\mu(t)
\]</div>
<p>But from the definition of <span class="math notranslate nohighlight">\(\mu(t)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
d\mu(t) = -a(t)\mu(t)\,dt \Rightarrow X(t)d\mu(t) = -a(t)\mu(t)X(t)\,dt
\]</div>
<p>So substituting <span class="math notranslate nohighlight">\(dX(t)\)</span> and <span class="math notranslate nohighlight">\(X(t)d\mu(t)\)</span> into the product rule:</p>
<div class="math notranslate nohighlight">
\[
d[\mu(t)X(t)] = \mu(t)\left[a(t)X(t) + b(t)\right]dt + \mu(t)\beta(t)\,dW_t - a(t)\mu(t)X(t)\,dt
\]</div>
<p>The <span class="math notranslate nohighlight">\(a(t)X(t)\)</span> terms cancel:</p>
<div class="math notranslate nohighlight">
\[
d[\mu(t)X(t)] = \mu(t)b(t)\,dt + \mu(t)\beta(t)\,dW_t
\]</div>
<p>Integrating both sides from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mu(t)X(t) = X_0 + \int_0^t \mu(s)b(s)\,ds + \int_0^t \mu(s)\beta(s)\,dW_s
\]</div>
<p>Multiplying both sides by <span class="math notranslate nohighlight">\(\mu(t)^{-1} = \exp\left(\int_0^t a(s)\,ds\right)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
X(t) = X_0 \exp\left( \int_0^t a(s)\,ds \right) + \exp\left( \int_0^t a(s)\,ds \right) \int_0^t \mu(s)b(s)\,ds + \exp\left( \int_0^t a(s)\,ds \right) \int_0^t \mu(s)\beta(s)\,dW_s
\]</div>
<p>Each term is now a linear combination of deterministic functions and a stochastic integral with deterministic integrand.</p>
<p>From the solution, <span class="math notranslate nohighlight">\(X(t)\)</span> can be written as:</p>
<div class="math notranslate nohighlight">
\[
X(t) = \text{Deterministic part} + \exp\left( \int_0^t a(s)ds \right)\int_0^t \mu(s)\beta(s)dW_s
\]</div>
<p>The stochastic part is:</p>
<div class="math notranslate nohighlight">
\[
\exp\left( \int_0^t a(s)ds \right)\int_0^t \mu(s)\beta(s)dW_s
\]</div>
<p>This is a stochastic integral of a deterministic function, hence:</p>
<ul class="simple">
<li><p>It is a Gaussian random variable.</p></li>
<li><p>It has mean zero.</p></li>
<li><p>Its variance is given:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\operatorname{Var}\left( \exp\left( \int_0^t a(s)ds \right)\int_0^t \mu(s)\beta(s)dW_s \right) = \exp\left( 2\int_0^t a(s)ds \right) \int_0^t \left[\exp\left( -\int_0^t a(s) ds \right)\beta(s)\right]^2\,ds
\]</div>
<p>Hence <span class="math notranslate nohighlight">\(X(t)\)</span> is a linear transformation of a Gaussian random variable, plus deterministic terms. Therefore:</p>
<p><span class="math notranslate nohighlight">\(X(t) \mid X_0\)</span> is normally distributed for all <span class="math notranslate nohighlight">\(t\)</span>, i.e., Gaussian.</p>
<p>So , If <span class="math notranslate nohighlight">\(f(x,t) = a(t)x + b(t)\)</span> and <span class="math notranslate nohighlight">\(\beta(x,t) = \beta(t)\)</span> (i.e., independent of <span class="math notranslate nohighlight">\(x\)</span>), then the SDE:</p>
<div class="math notranslate nohighlight">
\[
dX(t) = f(X,t)dt + \beta(t)dW_t = [a(t)X(t) + b(t)]dt + \beta(t)dW_t
\]</div>
<p>has solution <span class="math notranslate nohighlight">\(X(t) \mid X_0\)</span> that is Gaussian-distributed for all <span class="math notranslate nohighlight">\(t\)</span>, since the solution is a linear function of a Gaussian process.</p>
</section>
<section id="reverse-time-equation">
<h2>Reverse Time Equation<a class="headerlink" href="#reverse-time-equation" title="Link to this heading">#</a></h2>
<p>We saw OU process graph, we can see that after some time <span class="math notranslate nohighlight">\(T\)</span> the final state is a sample from a normal distribution. Now we can ask an interesting question here, given that we have the final state(a sample from normal distribution), can we trace back and reach the initial state? That is can we traverse back in time to reach a state from the initial distribution?
Or more broadly for a stochastic process we have the forward equation given by :</p>
<div class="math notranslate nohighlight">
\[ 
dX(t) = \mu(t)dt + \sigma(t)dW_t 
\]</div>
<p>for some initial state <span class="math notranslate nohighlight">\(X_0\)</span>. If we let this proces continue for some time <span class="math notranslate nohighlight">\(T\)</span> and let the state be now <span class="math notranslate nohighlight">\(X_T\)</span>. Can go back in reverse direction? Can we start from <span class="math notranslate nohighlight">\(X_T\)</span> a state from the final distribution and go back to a state from the intial distribution? The answer to that is YES!!.</p>
<p>The reverse time equation allows us to do this, which is given by:</p>
<div class="math notranslate nohighlight">
\[
dX(t) = \left(\mu(t) - \sigma^2(t)\nabla_X \log p(X(t),t)\right)dt + \sigma(t)d\hat{W}_t
\]</div>
<p>So by following this reverse SDE one can go back to a state from the initial  distribution.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="00_landing_page.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Blogs Landing Page</p>
      </div>
    </a>
    <a class="right-next"
       href="02_Diffusion_models_Score_Models_ETC.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Viewing Diffusion, Score, Rectified flow, Heirrachical VAEs From The Same Lens</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-spaces-and-random-variables">Probability Spaces And Random Variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-process">Stochastic Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-time-weiner-process">Discrete Time Weiner Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walk">Random walk</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-symmetric-random-walk">Scaled Symmetric Random Walk</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-interesting-case-arises-as-n-to-infty"><strong>A Interesting Case Arises As <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weiner-process">Weiner process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#total-variation">Total Variation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-variation">Quadratic Variation.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ito-s-lemma">ITO’s Lemma</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ito-s-product-rule">ITO’s Product Rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ornstein-uhlenbeck-process">Ornstein-Uhlenbeck Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ornsteinuhlenbeck-ou-process-solution">Ornstein–Uhlenbeck (OU) Process Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-of-x-t">Mean of <span class="math notranslate nohighlight">\(X_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-of-x-t">Variance of <span class="math notranslate nohighlight">\(X_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code">Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph">Graph</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-process-with-affine-drift-co-efficients">Stochastic Process With Affine Drift Co-efficients.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reverse-time-equation">Reverse Time Equation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yoghes and The Internet
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>