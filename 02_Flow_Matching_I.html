
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Blog 02: Flow Matching: The Theory Behind Stable Diffusion - 3. &#8212; Blogs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '02_Flow_Matching_I';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Blog 02: Diffusion Models" href="03_Diffusion_models_I.html" />
    <link rel="prev" title="Blog 01: SDE, Weiner Process, ITO’s Lemma and Reverse Time Equation" href="01_weiner_process.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_landing_page.html">
  
  
  
  
  
  
    <p class="title logo__title">Blogs</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_landing_page.html">
                    Blogs Landing Page
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_weiner_process.html">Blog 01: SDE, Weiner Process, ITO’s Lemma and Reverse Time Equation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Blog 02: Flow Matching: The Theory Behind Stable Diffusion - 3.</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Diffusion_models_I.html">Blog 02: Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Diffusion_models_II.html">Blog 03: Viewing Diffusion, Score, Rectified flow, Heirrachical VAEs From The Same Lens</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/yogheswaran-a/blogs" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/yogheswaran-a/blogs/issues/new?title=Issue%20on%20page%20%2F02_Flow_Matching_I.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/02_Flow_Matching_I.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Blog 02: Flow Matching: The Theory Behind Stable Diffusion - 3.</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-post-about">What is the post about?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-target">Marginal Target</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-matching-constructing-the-loss-function">Flow Matching: Constructing the Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-conditional-path">Gaussian Conditional Path</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-flow-matching-loss-for-gaussian-conditional-probability">Conditional Flow Matching Loss For Gaussian conditional Probability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#guidance">Guidance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-code">training code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuity-equation">Continuity Equation</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="blog-02-flow-matching-the-theory-behind-stable-diffusion-3">
<h1>Blog 02: Flow Matching: The Theory Behind Stable Diffusion - 3.<a class="headerlink" href="#blog-02-flow-matching-the-theory-behind-stable-diffusion-3" title="Link to this heading">#</a></h1>
<p>Most of the posts start with a pic or a qoute, I’m going to start with a lame joke.</p>
<p>A gaussian noise and MINST dataset fell in love with each other. How did they meet?<br />
They used flow matching app.</p>
<section id="what-is-the-post-about">
<h2>What is the post about?<a class="headerlink" href="#what-is-the-post-about" title="Link to this heading">#</a></h2>
<p>Diffusion models and flow matching have improved image generation(they both can be wriiten under the same formulation). In this blog post I will write my learnings about flow matching from the ground up, which was used to develop SD3, open ai SORA, Meta’s movie gen video, etc. The topis covered are:</p>
<ol class="arabic simple">
<li><p>Flow matching theory with self contained proofs.</p></li>
<li><p>Guidance.</p></li>
<li><p>Training a model on MINST dataset.</p></li>
</ol>
<p>I think the MIT’s course <a class="reference external" href="https://diffusion.csail.mit.edu/">Introduction to Flow Matching and Diffusion Models</a> (thanks a lot!!) is much better than what I have written, I learned from it, please take a look at it, they have notes, codes and video lectures. This post is inspired from it.</p>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>We want to generate images belonging to <span class="math notranslate nohighlight">\(P_{data}\)</span>. How can we go about this? Suppose we have some input <span class="math notranslate nohighlight">\(X_0\)</span> belonging to some initial distribution <span class="math notranslate nohighlight">\(P_0\)</span>. At time <span class="math notranslate nohighlight">\(t = T\)</span>, we want <span class="math notranslate nohighlight">\(X_T \sim P_{data}\)</span>. So at each time step we follow a path which connects initial distribution <span class="math notranslate nohighlight">\(P_0\)</span> and final distribution <span class="math notranslate nohighlight">\(P_{data}\)</span>. We can write the change of <span class="math notranslate nohighlight">\(X_t\)</span> as follows,</p>
<div class="math notranslate nohighlight">
\[
dX_t = U_t^{target}(X_t)\ dt
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
X_0 \sim P_0 \quad , X_t \sim P_t \ , \text{and} \quad X_T \sim P_{data}
\]</div>
<p>We will often restrict the time to lie between 0 and 1, i.e, <span class="math notranslate nohighlight">\(0\le t \le 1\)</span>.<br />
Now the problem simplifies (or becomes complex, we don’t know yet) to finding good <span class="math notranslate nohighlight">\(U_t^{target}(X_t)\)</span>. If we know <span class="math notranslate nohighlight">\(U_t^{target}(X_t)\)</span>, we can iteratively do the below,</p>
<div class="math notranslate nohighlight">
\[
X_{t + \Delta t} = X_t + U_t^{target}(X_t) \Delta t \ \text{; for t starting from 0 and ending at 1.}
\]</div>
<p>This is what flow matching is all about, finding a good <span class="math notranslate nohighlight">\(U_t^{target}(X_t)\)</span>. Thats it the post is over.</p>
<p>I always try to ask questions like how does one come up with such any idea or formulation? what could have motivated them to? Those who know Stochastic differential equations might have an idea about this. The langevin dynamics,</p>
<div class="math notranslate nohighlight">
\[ 
dX(t) = \mu(X,t)dt + \sigma(X,t)dW_t 
\]</div>
<p>converges to a static distribution (more curious readers can read my previous blog about <a class="reference external" href="https://yogheswaran-a.github.io/blogs/01_weiner_process.html#stochastic-process-with-affine-drift-co-efficients">stochastic-process-with-affine-drift-co-efficients</a>). We can use this, We can constuct a process such that the initial distribution can be the data distribution, the final can be a normal distribution. We reverse this using reverse time equation (read more on <a class="reference external" href="https://yogheswaran-a.github.io/blogs/01_weiner_process.html#reverse-time-equation">revrese time equation</a>)</p>
<div class="math notranslate nohighlight">
\[
\boxed{
dX(t) = \left[\mu(X,t) - \sigma(X,t)\sigma(X,t)^\top \nabla_X \log p(X,t)\right]dt + \sigma(X,t)\,d\hat{W}_t
}
\]</div>
<p>to go from known normal distribution to the data distribution. But this is an SDE process, flow matching which I described above is an ODE, so where is the motivation? In the lagevian dynamics if we put <span class="math notranslate nohighlight">\(\sigma(X,t) = 0\)</span> we get</p>
<div class="math notranslate nohighlight">
\[ 
dX(t) = \mu(X,t)dt + 0 * dW_t 
\]</div>
<p>the corresponding ODE, which follows the same probability distribution as <span class="math notranslate nohighlight">\(P_t\)</span> for <span class="math notranslate nohighlight">\(0\le t \le 1\)</span> as the SDE. This could be one of the motivations behind flow matching.</p>
</section>
<section id="marginal-target">
<h2>Marginal Target<a class="headerlink" href="#marginal-target" title="Link to this heading">#</a></h2>
<p>We need to learn <span class="math notranslate nohighlight">\(U_t^{target}(X_t)\)</span>, but we don’t have the ground truth value. So how do we approach this? We will make use of <span class="math notranslate nohighlight">\(U_t^{target}(X_t/Z)\)</span>, where <span class="math notranslate nohighlight">\(Z\)</span> is a data point from the dataset.  <span class="math notranslate nohighlight">\(U_t^{target}(X_t/Z)\)</span> can be made to have a simple analytical form, as we will see we can derive  <span class="math notranslate nohighlight">\(U_t^{target}(X_t)\)</span> from  <span class="math notranslate nohighlight">\(U_t^{target}(X_t/Z)\)</span>.</p>
<p>Let,</p>
<p><span class="math notranslate nohighlight">\(z \sim p_{data}\)</span>, <span class="math notranslate nohighlight">\(x_t \sim p_t(x/z)\)</span>.</p>
<p><span class="math notranslate nohighlight">\(p_0(x/z)\)</span> the initial distribution is noise, a normal distribution.<br />
<span class="math notranslate nohighlight">\(p_1(x/z)\)</span> the final distribution is a diarc delta centered around <span class="math notranslate nohighlight">\(z\)</span>, <span class="math notranslate nohighlight">\(\delta_z = \delta(x-z)\)</span>.</p>
<p>Then it follows that <span class="math notranslate nohighlight">\(p_1(x) \sim p_{data}\)</span></p>
<div class="math notranslate nohighlight">
\[
p_1(x \mid z) = \delta(x-z)
\]</div>
<p>and the marginal at time <span class="math notranslate nohighlight">\(t = 1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
p_1(x) = \int p_1(x \mid z) \, p_{\text{data}}(z) \, dz
\]</div>
<div class="math notranslate nohighlight">
\[
=&gt; p_1(x) = \int \delta(x - z) \, p_{\text{data}}(z) \, dz
\]</div>
<p>by property of diarc delta function, the above integral is zero everywhere except <span class="math notranslate nohighlight">\(x = z\)</span>, where it’s value is <span class="math notranslate nohighlight">\(p_{data}(z = x)\)</span></p>
<div class="math notranslate nohighlight">
\[
p_1(x) = p_{\text{data}}(x)
\]</div>
<p>Now, we can recover <span class="math notranslate nohighlight">\(U_t^{target}(X_t)\)</span> from  <span class="math notranslate nohighlight">\(U_t^{target}(X_t/Z)\)</span> using the below.</p>
<blockquote>
<div><h3 class="rubric" id="theorem-1-marginal-vector-field-property">Theorem 1: Marginal Vector Field Property</h3>
<p>For every data point <span class="math notranslate nohighlight">\(z \in \mathbb{R}^d\)</span>, let <span class="math notranslate nohighlight">\(u_t^{\text{target}}(z)\)</span> denote a conditional vector field, defined so that the corresponding ODE yields the conditional probability path <span class="math notranslate nohighlight">\(p_t(\cdot|z)\)</span>, viz.,</p>
<p><span class="math notranslate nohighlight">\( X_0 \sim p_{\text{init}}, \quad \frac{d}{dt}X_t = u_t^{\text{target}}(X_t|z) \implies X_t \sim p_t(\cdot|z) \quad (0 \le t \le 1).\)</span></p>
<p>Then the <strong>marginal vector field</strong> <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x)\)</span>, defined by</p>
<p><span class="math notranslate nohighlight">\(u_t^{\text{target}}(x) = \int u_t^{\text{target}}(x|z) \frac{p_t(x|z)p_{\text{data}}(z)}{p_t(x)}dz,\)</span></p>
<p>follows the marginal probability path, i.e.</p>
<div class="math notranslate nohighlight">
\[X_0 \sim p_{\text{init}}, \quad \frac{d}{dt}X_t = u_t^{\text{target}}(X_t) \implies X_t \sim p_t \quad (0 \le t \le 1).\]</div>
<p>In particular, <span class="math notranslate nohighlight">\(X_1 \sim p_{\text{data}}\)</span> for this ODE, so that we might say “<span class="math notranslate nohighlight">\(u_t^{\text{target}}\)</span> converts noise <span class="math notranslate nohighlight">\(p_{\text{init}}\)</span> into data <span class="math notranslate nohighlight">\(p_{\text{data}}\)</span>”.</p>
</div></blockquote>
<p>Proof:</p>
<p><span class="math notranslate nohighlight">\(p_t(x)\)</span> can be written as:</p>
<div class="math notranslate nohighlight">
\[
p_t(x) = \int p_t(x, z)\, dz = \int p_t(x | z) \, p_{\text{data}}(z) \, dz.
\]</div>
<p>For the conditional distribution, the continuity equation (proof in appendix) holds:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial t} p_t(x | z) = -\nabla \cdot (p_t(x | z)\, u_t^{\text{target}}(x | z)),
\]</div>
<p>Multiply both sides by <span class="math notranslate nohighlight">\(p_{\text{data}}(z)\)</span>, and integrate over <span class="math notranslate nohighlight">\(z\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\int \frac{\partial}{\partial t} p_t(x | z) \, p_{\text{data}}(z) dz = \int -\nabla \cdot \left(p_t(x | z)\, u_t^{\text{target}}(x | z)\right) p_{\text{data}}(z) dz.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(p_{\text{data}}(z)\)</span> is independent of <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(x\)</span>, we can move the partial derivative with respect to <span class="math notranslate nohighlight">\(t\)</span> outside the integral on the LHS, and the divergence operator outside the integral on the RHS:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial t} \int p_t(x | z) \, p_{\text{data}}(z) dz = -\nabla \cdot \int p_t(x | z)\, u_t^{\text{target}}(x | z)\, p_{\text{data}}(z) dz.
\]</div>
<p>Using <span class="math notranslate nohighlight">\(\int p_t(x | z) \, p_{\text{data}}(z) dz = p_t(x)\)</span>. The LHS can be written  as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial t} p_t(x) = -\nabla \cdot \int p_t(x | z)\, u_t^{\text{target}}(x | z)\, p_{\text{data}}(z) dz.
\]</div>
<p>For the marginal distribution <span class="math notranslate nohighlight">\(p_t(x)\)</span> to evolve according to <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x)\)</span> via the continuity equation, it must satisfy:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial t} p_t(x) = -\nabla \cdot (p_t(x) u_t^{\text{target}}(x)).
\]</div>
<p>Comparing the two expressions for <span class="math notranslate nohighlight">\(\frac{\partial}{\partial t} p_t(x)\)</span>, we must have:</p>
<div class="math notranslate nohighlight">
\[
p_t(x) u_t^{\text{target}}(x) = \int p_t(x | z)\, u_t^{\text{target}}(x | z)\, p_{\text{data}}(z) dz.
\]</div>
<p>Now, define the marginal vector field <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
u_t^{\text{target}}(x) := \frac{1}{p_t(x)} \int p_t(x | z)\, u_t^{\text{target}}(x | z)\, p_{\text{data}}(z) dz,
\]</div>
<p>which simplifies to:</p>
<div class="math notranslate nohighlight">
\[
u_t^{\text{target}}(x) = \int u_t^{\text{target}}(x | z)\, \frac{p_t(x | z)\, p_{\text{data}}(z)}{p_t(x)} dz.
\]</div>
<p>Therefore, If <span class="math notranslate nohighlight">\(X_t \sim p_t(\cdot | z)\)</span> evolves under the conditional vector field <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x | z)\)</span>, then the marginal vector field</p>
<div class="math notranslate nohighlight">
\[
u_t^{\text{target}}(x) = \int u_t^{\text{target}}(x | z) \frac{p_t(x | z)\, p_{\text{data}}(z)}{p_t(x)} dz
\]</div>
<p>satisfies the continuity equation for the marginal <span class="math notranslate nohighlight">\(p_t(x)\)</span>. This proves the desired result.</p>
</section>
<section id="flow-matching-constructing-the-loss-function">
<h2>Flow Matching: Constructing the Loss Function<a class="headerlink" href="#flow-matching-constructing-the-loss-function" title="Link to this heading">#</a></h2>
<p>So far we have gained some insight into how to construct <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x)\)</span>. We will use NN <span class="math notranslate nohighlight">\(u_t^\theta\)</span> to apprx <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x)\)</span>.</p>
<p>Let,</p>
<div class="math notranslate nohighlight">
\[
 X_0 \sim p_{\text{init}}, \quad \frac{d}{dt}X_t = u_t^{\text{target}}(X_t) \implies X_t \sim p_t(x_t) \quad (0 \le t \le 1).
 \]</div>
<p>We can learn <span class="math notranslate nohighlight">\(u_t^\theta\)</span> by minimizing the mean squared loss,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \text{Unif[0,1]}, x \sim p_t} \left[\|u_{\theta t}(x) - u_t^{\text{target}}(x)\|^2\right] \\
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \text{Unif[0,1]}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)}\left[\|u_{\theta t}(x) - u_t^{\text{target}}(x)\|^2\right],
\end{split}\]</div>
<p>The above is called the marginal flow matching loss.
What this says is, First, draw a random time <span class="math notranslate nohighlight">\(t \in [0, 1]\)</span>. Second, draw a random point <span class="math notranslate nohighlight">\(z\)</span> from our data set, sample from <span class="math notranslate nohighlight">\(p_t(\cdot|z)\)</span> (e.g., by adding some noise), and compute <span class="math notranslate nohighlight">\(u_{\theta t}(x)\)</span>. Finally, compute the mean-squared error between the output of our neural network and the marginal vector field <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x)\)</span>. But here the problem is we do not know <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
u_t^{\text{target}}(x) = \int u_t^{\text{target}}(x | z)\, \frac{p_t(x | z)\, p_{\text{data}}(z)}{p_t(x)} dz.
\]</div>
<p>The above is not tracable. So what can we do? The idea is to realise we only need gradient of the loss function to update the parameters and ask ourselves what would happen if we replaced <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x)\)</span> with <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x/z)\)</span> in <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{FM}}(\theta)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)}\left[\|u_{\theta t}(x) - u_t^{\text{target}}(x|z)\|^2\right].
\]</div>
<p>This is called the conditional flow matching loss.
For the above can we get any bound? If its an upper bound it would be great, ie <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{FM}}(\theta) \le \mathcal{L}_{\text{CFM}}(\theta)\)</span>. As you might have guessed, yes we can derive a bound, more specifically the two terms are equal upto a constant term, so thier gradients are equal.</p>
<blockquote>
<div><h3 class="rubric" id="theorem-2">Theorem 2</h3>
<p>The marginal flow matching loss equals the conditional flow matching loss up to a constant. That is,</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{FM}}(\theta) = \mathcal{L}_{\text{CFM}}(\theta) + C,\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is independent of <span class="math notranslate nohighlight">\(\theta\)</span>. Therefore, their gradients coincide:</p>
<div class="math notranslate nohighlight">
\[\nabla_\theta \mathcal{L}_{\text{FM}}(\theta) = \nabla_\theta \mathcal{L}_{\text{CFM}}(\theta).\]</div>
<p>Hence, minimizing <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CFM}}(\theta)\)</span> with e.g., stochastic gradient descent (SGD) is equivalent to minimizing <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{FM}}(\theta)\)</span> with in the same fashion. In particular, for the minimizer <span class="math notranslate nohighlight">\(\theta^*\)</span> of <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CFM}}(\theta)\)</span>, it will hold that <span class="math notranslate nohighlight">\(u_t^{\theta*} = u_t^{\text{target}}\)</span>.</p>
</div></blockquote>
<p><strong>Proof:</strong></p>
<p>We expand the mean-squared error into three components and remove constants.</p>
<p>The marginal flow matching loss, <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{FM}}(\theta)\)</span>, is defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, x \sim p_t} \left[\|u_t^{\theta}(x) - u_t^{\text{target}}(x)\|^2\right]\]</div>
<p>Expanding the squared Euclidean norm <span class="math notranslate nohighlight">\(\|a-b\|^2 = \|a\|^2 - 2a^T b + \|b\|^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, x \sim p_t} \left[\|u_t^{\theta}(x)\|^2 - 2u_t^{\theta}(x)^T u_t^{\text{target}}(x) + \|u_t^{\text{target}}(x)\|^2\right]
\]</div>
<p>Separating the expectation and defining a constant <span class="math notranslate nohighlight">\(C_1 = \mathbb{E}_{t \sim \text{Unif}, x \sim p_t} [\|u_t^{\text{target}}(x)\|^2]\)</span> which is independent of <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, x \sim p_t} \left[\|u_t^{\theta}(x)\|^2 - 2u_t^{\theta}(x)^T u_t^{\text{target}}(x)\right] + C_1
\]</div>
<p>By using the sampling procedure of <span class="math notranslate nohighlight">\(p_t\)</span> (which involves marginalizing over <span class="math notranslate nohighlight">\(z\)</span> from <span class="math notranslate nohighlight">\(p_{\text{data}}\)</span> and <span class="math notranslate nohighlight">\(p_t(\cdot|z)\)</span>), the expectation can be rewritten:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)} \left[\|u_t^{\theta}(x)\|^2 - 2u_t^{\theta}(x)^T u_t^{\text{target}}(x)\right] + C_1
\]</div>
<p>Next, let us re-express the second summand, <span class="math notranslate nohighlight">\(\mathbb{E}_{t \sim \text{Unif}, x \sim p_t} [u_t^{\theta}(x)^T u_t^{\text{target}}(x)]\)</span>.
By definition of the expectation over <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{t \sim \text{Unif}, x \sim p_t} [u_t^{\theta}(x)^T u_t^{\text{target}}(x)] = \int_0^1 \int p_t(x) u_t^{\theta}(x)^T u_t^{\text{target}}(x) \, dx \, dt
\]</div>
<p>Substitute the definition of the marginal vector field <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x) = \int u_t^{\text{target}}(x | z) \frac{p_t(x | z)p_{\text{data}}(z)}{p_t(x)} dz\)</span>:</p>
<div class="math notranslate nohighlight">
\[
= \int_0^1 \int p_t(x) u_t^{\theta}(x)^T \left[\int u_t^{\text{target}}(x | z) \frac{p_t(x | z)p_{\text{data}}(z)}{p_t(x)} dz\right] \, dx \, dt
\]</div>
<p>The <span class="math notranslate nohighlight">\(p_t(x)\)</span> terms cancel, and by changing order of integration:</p>
<div class="math notranslate nohighlight">
\[
= \int_0^1 \int \int u_t^{\theta}(x)^T u_t^{\text{target}}(x | z) p_t(x | z) p_{\text{data}}(z) \, dz \, dx \, dt
\]</div>
<p>This integral can be re-expressed as an expectation over the relevant distributions:</p>
<div class="math notranslate nohighlight">
\[
= \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)} [u_t^{\theta}(x)^T u_t^{\text{target}}(x | z)]
\]</div>
<p>We plug the conditional vector field <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x|z)\)</span> into the equation for <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{FM}}\)</span> to get:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)} \left[\|u_t^{\theta}(x)\|^2 - 2\mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)} [u_t^{\theta}(x)^T u_t^{\text{target}}(x|z)] \right] + C_1
\]</div>
<p>By adding and subtracting <span class="math notranslate nohighlight">\(\|u_t^{\text{target}}(x|z)\|^2\)</span> inside the expectation, and regrouping terms:</p>
<div class="math notranslate nohighlight">
\[
= \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)} \left[\|u_t^{\theta}(x)\|^2 - 2u_t^{\theta}(x)^T u_t^{\text{target}}(x|z) + \|u_t^{\text{target}}(x|z)\|^2 - \|u_t^{\text{target}}(x|z)\|^2 \right] + C_1
\]</div>
<p>This allows us to form the squared difference <span class="math notranslate nohighlight">\(\|u_t^{\theta}(x) - u_t^{\text{target}}(x|z)\|^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
= \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)} \left[\|u_t^{\theta}(x) - u_t^{\text{target}}(x|z)\|^2 \right] + \underbrace{\mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)} [-\|u_t^{\text{target}}(x|z)\|^2]}_{C_2} + C_1
\]</div>
<p>Recognizing the first term as <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CFM}}(\theta)\)</span> and combining the constants <span class="math notranslate nohighlight">\(C_1\)</span> and <span class="math notranslate nohighlight">\(C_2\)</span> into a single constant <span class="math notranslate nohighlight">\(C\)</span>:</p>
<div class="math notranslate nohighlight">
\[
= \mathcal{L}_{\text{CFM}}(\theta) + \underbrace{C_2 + C_1}_{=C}
\]</div>
<p>This concludes the proof.</p>
<p>We can train the <span class="math notranslate nohighlight">\(u_t^{\theta}(x)\)</span>, we can iterate using the below to get samples.</p>
<div class="math notranslate nohighlight">
\[
dX_t = U_{\theta}^t (X_t) dt, \quad X_0 \sim p_{\text{init}}
\]</div>
<p>This procedure is called <strong>Flow Matching</strong>, which is summarized below.</p>
<blockquote>
<div><h3 class="rubric" id="algorithm-1-flow-matching-training-procedure">Algorithm 1: Flow Matching Training Procedure</h3>
<p><strong>Require:</strong> A dataset of samples <span class="math notranslate nohighlight">\(z \sim p_{data}\)</span>, neural network <span class="math notranslate nohighlight">\(u_t^\theta\)</span></p>
<ol class="arabic simple">
<li><p><strong>for each</strong> mini-batch of data <strong>do</strong></p></li>
<li><p>    Sample a data example <span class="math notranslate nohighlight">\(z\)</span> from the dataset.</p></li>
<li><p>    Sample a random time <span class="math notranslate nohighlight">\(t \sim \text{Unif}_{[0,1]}\)</span>.</p></li>
<li><p>    Sample <span class="math notranslate nohighlight">\(x \sim p_t(\cdot|z)\)</span></p></li>
<li><p>    Compute loss
<span class="math notranslate nohighlight">\( \mathcal{L}(\theta) = \|u_t^\theta(x) - u_t^{\text{target}}(x|z)\|^2\)</span>
    </p></li>
<li><p>    Update the model parameters <span class="math notranslate nohighlight">\(\theta\)</span> via gradient descent on <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p></li>
<li><p><strong>end for</strong></p></li>
</ol>
</div></blockquote>
<p>Things simplify we when assume the conditional probability <span class="math notranslate nohighlight">\(x \sim p_t(\cdot|z)\)</span> is a gaussian, the SD-3, meta’s movie gen are trainined using gaussian conditional path. Lets see what the loss looks like for this case.</p>
</section>
<section id="gaussian-conditional-path">
<h2>Gaussian Conditional Path<a class="headerlink" href="#gaussian-conditional-path" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\alpha_t\)</span>, <span class="math notranslate nohighlight">\(\beta_t\)</span> be noise schedulers: two continuously differentiable, monotonic functions with <span class="math notranslate nohighlight">\(\alpha_0 = \beta_1 = 0\)</span> and <span class="math notranslate nohighlight">\(\alpha_1 = \beta_0 = 1\)</span>. The gaussian conditional path is defined as:</p>
<div class="math notranslate nohighlight">
\[p_t(\cdot|z) = \mathcal{N}(\alpha_t z, \beta_t^2 I_d)\]</div>
<p>By the conditions we imposed on <span class="math notranslate nohighlight">\(\alpha_t\)</span> and <span class="math notranslate nohighlight">\(\beta_t\)</span>,</p>
<p><span class="math notranslate nohighlight">\(p_0(\cdot|z) = \mathcal{N}(\alpha_0z, \beta_0^2 I_d) = \mathcal{N}(0, I_d)\)</span>, and <span class="math notranslate nohighlight">\(p_1(\cdot|z) = \mathcal{N}(\alpha_1z, \beta_1^2 I_d) = \delta_z \)</span></p>
<p>We can sample <span class="math notranslate nohighlight">\(x\)</span> from <span class="math notranslate nohighlight">\(p(\cdot|z)\)</span> as follows,</p>
<p><span class="math notranslate nohighlight">\(z \sim p_{data}\)</span>, <span class="math notranslate nohighlight">\(\epsilon \sim p_{init} = \mathcal{N}(0, I_d) \Rightarrow x = \alpha_t z + \beta_t \epsilon \sim p_t\)</span></p>
<p>Intuitively, the above procedure adds more noise as <span class="math notranslate nohighlight">\(t\)</span> goes to <span class="math notranslate nohighlight">\(t = 0\)</span> from <span class="math notranslate nohighlight">\(T\)</span>, at <span class="math notranslate nohighlight">\(t = 0\)</span> there is only noise.</p>
<p>Now to construct <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CFM}}(\theta)\)</span> loss we need <span class="math notranslate nohighlight">\(u_t^{\text{target}}(x|z)\)</span>, lets derive it.</p>
<blockquote>
<div><h3 class="rubric" id="theorem-3">Theorem 3:</h3>
<p>For <span class="math notranslate nohighlight">\(p_t(\cdot|z) = \mathcal{N}(\alpha_tz, \beta_t^2 I_d)\)</span>, with <span class="math notranslate nohighlight">\(\alpha_0 = \beta_1 = 0\)</span> and <span class="math notranslate nohighlight">\(\alpha_1 = \beta_0 = 1\)</span>.. The conditional target is given by:</p>
<div class="math notranslate nohighlight">
\[u_t^{\text{target}}(x|z) = \frac{\dot{\alpha}_t - \dot{\beta}_t}{\beta_t} \alpha_t z + \frac{\dot{\beta}_t}{\beta_t} x\]</div>
</div></blockquote>
<p><strong>Proof</strong></p>
<p>Let <span class="math notranslate nohighlight">\(X_t\)</span> be defined as:</p>
<div class="math notranslate nohighlight">
\[
X_t = \alpha_t z + \beta_t X_0
\]</div>
<p>with <span class="math notranslate nohighlight">\(X_0 \sim p_{\text{init}} = \mathcal{N}(0, I_d)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
X_t = \alpha_t z + \beta_t X_0 \sim \mathcal{N}(\alpha_t z, \beta_t^2 I_d) = p_t(\cdot|z).
\]</div>
<p>and at t = 1, <span class="math notranslate nohighlight">\(\alpha_1 = 1\)</span>, <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span></p>
<div class="math notranslate nohighlight">
\[
X_1 \sim P_1 = P_{data}
\]</div>
<p>We conclude that the trajectories are distributed like the conditional probability path. Therefore</p>
<p>The <span class="math notranslate nohighlight">\(X_t\)</span> defined above is the ODE trajectory of</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt} X_t = u_{t}^{\text{target}}(x|z) \quad \text{for all } x, z \in \mathbb{R}^d
\]</div>
<p>Now lets evalute <span class="math notranslate nohighlight">\(u_{t}^{\text{target}}(x|z)\)</span> from the ODE,</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt} X_t = u_{t}^{\text{target}}(x|z) \quad \text{for all } x, z \in \mathbb{R}^d
\]</div>
<div class="math notranslate nohighlight">
\[
\Leftrightarrow \dot{\alpha}_t z + \dot{\beta}_t x_0 = u_{t}^{\text{target}}(\alpha_t z + \beta_t x|z) \quad \text{for all } x_0, z \in \mathbb{R}^d
\]</div>
<p>rewritting <span class="math notranslate nohighlight">\(x_0 =  (x_t - \alpha_t z) / \beta_t\)</span></p>
<div class="math notranslate nohighlight">
\[
\Leftrightarrow \dot{\alpha}_t z + \dot{\beta}_t \left( \frac{x_t - \alpha_t z}{\beta_t} \right) = u_{t}^{\text{target}}(x_t|z) \quad \text{for all } x_t, z \in \mathbb{R}^d
\]</div>
<p>simplyfing notations <span class="math notranslate nohighlight">\(x_t\)</span> as <span class="math notranslate nohighlight">\(x\)</span></p>
<div class="math notranslate nohighlight">
\[
\Leftrightarrow \left( \dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t \right) z + \frac{\dot{\beta}_t}{\beta_t} x = u_{t}^{\text{target}}(x|z) \quad \text{for all } x, z \in \mathbb{R}^d
\]</div>
<p>Which proves the theorem.</p>
<section id="conditional-flow-matching-loss-for-gaussian-conditional-probability">
<h3>Conditional Flow Matching Loss For Gaussian conditional Probability<a class="headerlink" href="#conditional-flow-matching-loss-for-gaussian-conditional-probability" title="Link to this heading">#</a></h3>
<p>As we have seen above we have <span class="math notranslate nohighlight">\(p_t(\cdot|z) = \mathcal{N}(\alpha_t z; \beta_t^2 I_d)\)</span>, we can sample from the conditional path via</p>
<div class="math notranslate nohighlight">
\[
x_0 = \epsilon \sim \mathcal{N}(0, I_d) \Rightarrow x_t = \alpha_t z + \beta_t \epsilon \sim \mathcal{N}(\alpha_t z, \beta_t^2 I_d) = p_t(\cdot|z)
\]</div>
<p>Using the theorem 3,  <span class="math notranslate nohighlight">\(u_{t}^{\text{target}}(x|z)\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
u_{t}^{\text{target}}(x|z) = \left( \dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t \right) z + \frac{\dot{\beta}_t}{\beta_t} x
\]</div>
<p>Plugging <span class="math notranslate nohighlight">\(u_{t}^{\text{target}}(x|z)\)</span> in <span class="math notranslate nohighlight">\(\mathcal{L}_{\text{CFM}}(\theta)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)}\left[\|u_{\theta t}(x) - u_t^{\text{target}}(x|z)\|^2\right].
\]</div>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, x \sim \mathcal{N}(\alpha_t z, \beta_t^2 I_d)}\left[ \left\| u_t^\theta(x) - \left( \left( \dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t \right) z + \frac{\dot{\beta}_t}{\beta_t} x \right) \right\|^2 \right]
\]</div>
<p>Substitue <span class="math notranslate nohighlight">\(x = \alpha_t z + \beta_t \epsilon\)</span></p>
<div class="math notranslate nohighlight">
\[
= \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, \epsilon \sim \mathcal{N}(0, I_d)}\left[ \left\| u_t^\theta(\alpha_t z + \beta_t \epsilon) - (\dot{\alpha}_t z + \dot{\beta}_t \epsilon) \right\|^2 \right]
\]</div>
<p>A special case of the above is when when we substitue  <span class="math notranslate nohighlight">\(\alpha_t = t\)</span>, and <span class="math notranslate nohighlight">\(\beta_t = 1 - t\)</span>.</p>
<p>Then we have <span class="math notranslate nohighlight">\(\dot{\alpha}_t = 1\)</span>, <span class="math notranslate nohighlight">\(\dot{\beta}_t = -1\)</span>, so that</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t \sim \text{Unif}, z \sim p_{\text{data}}, \epsilon \sim \mathcal{N}(0,I_d)}[\left\|u_t^\theta(tz + (1 - t)\epsilon) - (z - \epsilon)\right\|^2]
\]</div>
<p>Stable Diffusion 3 and Meta’s Movie Gen Video are trained using this loss function.</p>
<blockquote>
<div><h3 class="rubric" id="algorithm-2-flow-matching-training-procedure-for-gaussian-conditional-path">Algorithm 2: Flow Matching Training Procedure For Gaussian Conditional Path</h3>
<p><strong>Require:</strong> A dataset of samples <span class="math notranslate nohighlight">\(z \sim p_{data}\)</span>, neural network <span class="math notranslate nohighlight">\(u_t^\theta\)</span></p>
<ol class="arabic simple">
<li><p><strong>for each</strong> mini-batch of data <strong>do</strong></p></li>
<li><p>    Sample a data example <span class="math notranslate nohighlight">\(z\)</span> from the dataset.</p></li>
<li><p>    Sample a random time <span class="math notranslate nohighlight">\(t \sim \text{Unif}_{[0,1]}\)</span>.</p></li>
<li><p>    Sample noise <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, I_d)\)</span>.</p></li>
<li><p>    Set <span class="math notranslate nohighlight">\(x = tz + (1-t)\epsilon\)</span>               (General case: <span class="math notranslate nohighlight">\(x \sim p_t(\cdot|z)\)</span>)</p></li>
<li><p>    Compute loss
    
    <span class="math notranslate nohighlight">\(\mathcal{L}(\theta) = \|u_t^\theta(x) - (z - \epsilon)\|^2 \quad \)</span>(General case:<span class="math notranslate nohighlight">\( \)</span> <span class="math notranslate nohighlight">\(\mathcal{L}(\theta) = \|u_t^\theta(x) - u_t^{\text{target}}(x|z)\|^2)\)</span>
    </p></li>
<li><p>    Update the model parameters <span class="math notranslate nohighlight">\(\theta\)</span> via gradient descent on <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p></li>
<li><p><strong>end for</strong></p></li>
</ol>
</div></blockquote>
</section>
</section>
<section id="guidance">
<h2>Guidance<a class="headerlink" href="#guidance" title="Link to this heading">#</a></h2>
<p>TD</p>
</section>
<section id="training-code">
<h2>training code<a class="headerlink" href="#training-code" title="Link to this heading">#</a></h2>
<p>TD</p>
</section>
<section id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Link to this heading">#</a></h2>
<section id="continuity-equation">
<h3>Continuity Equation<a class="headerlink" href="#continuity-equation" title="Link to this heading">#</a></h3>
<p>TD</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_weiner_process.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Blog 01: SDE, Weiner Process, ITO’s Lemma and Reverse Time Equation</p>
      </div>
    </a>
    <a class="right-next"
       href="03_Diffusion_models_I.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Blog 02: Diffusion Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-post-about">What is the post about?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-target">Marginal Target</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-matching-constructing-the-loss-function">Flow Matching: Constructing the Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-conditional-path">Gaussian Conditional Path</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-flow-matching-loss-for-gaussian-conditional-probability">Conditional Flow Matching Loss For Gaussian conditional Probability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#guidance">Guidance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-code">training code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuity-equation">Continuity Equation</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yoghes and The Internet
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>